{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"Character_CNN_LSTM.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZPDDXI7bvm7a","colab_type":"code","outputId":"02df9f31-2cfe-44a8-d811-8b0736b3da78","colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1572416006568,"user_tz":-330,"elapsed":28217,"user":{"displayName":"Kushagra Bhatia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8m2Hqe_J_VckJ1PlnRCIhfLS2zsKUH5E0SzgRfA=s64","userId":"12476490699397159076"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_W-PihZewZO8","colab_type":"code","outputId":"6a3d819a-b2ef-40f5-d1da-ed6069729791","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1572416006585,"user_tz":-330,"elapsed":28181,"user":{"displayName":"Kushagra Bhatia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8m2Hqe_J_VckJ1PlnRCIhfLS2zsKUH5E0SzgRfA=s64","userId":"12476490699397159076"}}},"source":["cd /content/drive/My Drive/Colab Notebooks/HAN"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/HAN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5CjD9A9BeqLf","colab_type":"code","outputId":"68253d22-7cbc-4744-ab81-88ed80ac67e7","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1572416010553,"user_tz":-330,"elapsed":32085,"user":{"displayName":"Kushagra Bhatia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8m2Hqe_J_VckJ1PlnRCIhfLS2zsKUH5E0SzgRfA=s64","userId":"12476490699397159076"}}},"source":["ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Character_CNN_LSTM.ipynb  MyNormalizer.py  Untitled7.ipynb\n","IIITH_Codemixed.txt       \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BvehAKxA7GkV","colab_type":"code","colab":{}},"source":["\n","import numpy as np\n","import h5py\n","import pickle\n","from copy import deepcopy\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.preprocessing import sequence\n","from keras import backend as K\n","from keras.layers.core import Dense, Dropout, Activation\n","from keras.layers.embeddings import Embedding\n","from keras.layers.recurrent import LSTM, GRU\n","from keras.layers.convolutional import Convolution1D, MaxPooling1D\n","from keras.layers import Conv1D\n","import tensorflow as tf\n","\n","from keras.utils import np_utils\n","from MyNormalizer import token\n","\n","################# GLOBAL VARIABLES #####################\n","\n","inputdatasetfilename = 'IIITH_Codemixed.txt'\n","\n","\n","#Data I/O formatting\n","SEPERATOR = '\\t'\n","DATA_COLUMN = 1\n","LABEL_COLUMN = 3\n","LABELS = ['0','1','2'] # 0 -> Negative, 1-> Neutral, 2-> Positive\n","mapping_char2num = {}\n","mapping_num2char = {}\n","MAXLEN = 200\n","\n","#LSTM Model Parameters\n","#Embedding\n","MAX_FEATURES = 0\n","embedding_size = 128\n","# Convolution\n","filter_length = 3\n","nb_filter = 128\n","pool_length = 3\n","# LSTM\n","lstm_output_size = 128\n","# Training\n","batch_size = 128\n","number_of_epochs = 80\n","numclasses = 3\n","test_size = 0.2\n","########################################################\n","\n","def parse(filename,seperator,labelcol,labels,datacol):\n","\t\"\"\"\n","\tPurpose -> Data I/O\n","\tInput   -> Data file containing sentences and labels along with the global variables\n","\tOutput  -> Sentences cleaned up in list of lists format along with the labels as a numpy array\n","\t\"\"\"\n","\tf=open(filename,'r',encoding='utf-8')\n","\tlines = f.read().lower()\n","\tlines = lines.lower().split('\\n')[:-1]\n","\n","\tX_train = []\n","\tY_train = []\n","\t\n","\tfor line in lines:\n","\t\tline = line.split(seperator)\n","\t\ttokenized_lines = token(line[datacol])\n","\t\t\n","\t\tchar_list = []\n","\t\tfor words in tokenized_lines:\n","\t\t\tfor char in words:\n","\t\t\t\tchar_list.append(char)\n","\t\t\tchar_list.append(' ')\n","\t\t#print(char_list) - Debugs the character list created\n","\t\tX_train.append(char_list)\n","\t\t\n","\t\t#Appends labels\n","\t\tif line[labelcol] == labels[0]:\n","\t\t\tY_train.append(0)\n","\t\tif line[labelcol] == labels[1]:\n","\t\t\tY_train.append(1)\n","\t\tif line[labelcol] == labels[2]:\n","\t\t\tY_train.append(2)\n","\t\n","\t#Converts Y_train to a numpy array\t\n","\tY_train = np.asarray(Y_train)\n","\tassert(len(X_train) == Y_train.shape[0])\n","\n","\treturn [X_train,Y_train]\n","\n","def convert_char2num(mapping_n2c,mapping_c2n,trainwords,maxlen):\n","\t\"\"\"\n","\tPurpose -> Convert characters to integers, a unique value for every character\n","\tInput   -> Training data (In list of lists format) along with global variables\n","\tOutput  -> Converted training data along with global variables\n","\t\"\"\"\n","\tallchars = []\n","\terrors = 0\n","\n","\tfor line in trainwords:\n","\t\ttry:\n","\t\t\tallchars = set(allchars+line)\n","\t\t\tallchars = list(allchars)\n","\t\texcept:\n","\t\t\terrors += 1\n","\n","\tcharno = 0\n","\tfor char in allchars:\n","\t\tmapping_char2num[char] = charno\n","\t\tmapping_num2char[charno] = char\n","\t\tcharno += 1\n","\n","\tassert(len(allchars)==charno) \n","\n","\t#Converts the data from characters to numbers using dictionaries \n","\tX_train = []\n","\tfor line in trainwords:\n","\t\tchar_list=[]\n","\t\tfor letter in line:\n","\t\t\tchar_list.append(mapping_char2num[letter])\n","\t\tX_train.append(char_list)\n","\tprint(mapping_char2num)\n","\tprint(mapping_num2char)\n","\tX_train = sequence.pad_sequences(X_train[:], maxlen=maxlen)\n","\treturn [X_train,mapping_num2char,mapping_char2num,charno]\n","\n","\n","\n","def get_activations(model, layer, X_batch):\n","\t\"\"\"\n","\tPurpose -> Obtains outputs from any layer in Keras\n","\tInput   -> Trained model, layer from which output needs to be extracted & files to be given as input\n","\tOutput  -> Features from that layer \n","\t\"\"\"\n","\t#Referred from:- TODO: Enter the forum link from where I got this\n","\tget_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n","\tactivations = get_activations([X_batch,0])\n","\treturn activations\n","\n","def evaluate_model(X_test,y_test,model,batch_size,numclasses):\n","\t\"\"\"\n","\tPurpose -> Evaluate any model on the testing data\n","\tInput   -> Testing data and labels, trained model and global variables\n","\tOutput  -> Nil\n","\t\"\"\"\n","\t#Convert y_test to one-hot encoding\n","\ty_test = np_utils.to_categorical(y_test, numclasses)\n","\t#Evaluate the accuracies\n","\tscore, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n","\tprint('Test score:', score)\n","\tprint('Test accuracy:', acc)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQNC-ygp7Gmn","colab_type":"code","outputId":"915b2278-a6b8-408e-f4ab-7e95be6b1b40","colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1572416464266,"user_tz":-330,"elapsed":1259,"user":{"displayName":"Kushagra Bhatia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8m2Hqe_J_VckJ1PlnRCIhfLS2zsKUH5E0SzgRfA=s64","userId":"12476490699397159076"}}},"source":["    out = parse(inputdatasetfilename,SEPERATOR,LABEL_COLUMN,LABELS,DATA_COLUMN)\n","    global X_train\n","    global y_train\n","    global X_test\n","    global y_test\n","    X_train = out[0]\n","    y_train = out[1]\n","    print(y_train)\n","    print('Creating character dictionaries and format conversion in progess...')\n","    out = convert_char2num(mapping_num2char,mapping_char2num,X_train,MAXLEN)\n","    mapping_num2char = out[1]\n","    mapping_char2num = out[2]\n","    MAX_FEATURES = out[3]\n","    X_train = np.asarray(out[0])\n","    y_train = np.asarray(y_train).flatten()\n","    print('Splitting data into train and test...')\n","    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","    y_train=tf.keras.utils.to_categorical(y_train)\n","    y_test=tf.keras.utils.to_categorical(y_test)\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[2 2 1 ... 2 2 2]\n","Creating character dictionaries and format conversion in progess...\n","{'x': 0, 'i': 1, 'y': 2, 'v': 3, 't': 4, ' ': 5, 's': 6, 'm': 7, 'f': 8, 'p': 9, 'n': 10, 'd': 11, 'r': 12, 'z': 13, 'q': 14, 'g': 15, 'c': 16, 'u': 17, 'h': 18, 'j': 19, 'b': 20, 'o': 21, 'e': 22, 'w': 23, 'k': 24, 'l': 25, 'a': 26}\n","{0: 'x', 1: 'i', 2: 'y', 3: 'v', 4: 't', 5: ' ', 6: 's', 7: 'm', 8: 'f', 9: 'p', 10: 'n', 11: 'd', 12: 'r', 13: 'z', 14: 'q', 15: 'g', 16: 'c', 17: 'u', 18: 'h', 19: 'j', 20: 'b', 21: 'o', 22: 'e', 23: 'w', 24: 'k', 25: 'l', 26: 'a'}\n","Splitting data into train and test...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m-QD8NwuCtQ8","colab_type":"code","outputId":"43608d15-cb03-4ef2-bd7e-6acf3f79d574","colab":{"base_uri":"https://localhost:8080/","height":197},"executionInfo":{"status":"ok","timestamp":1572416477365,"user_tz":-330,"elapsed":7159,"user":{"displayName":"Kushagra Bhatia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8m2Hqe_J_VckJ1PlnRCIhfLS2zsKUH5E0SzgRfA=s64","userId":"12476490699397159076"}}},"source":["!pip install scikit-optimize"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Collecting scikit-optimize\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n","\r\u001b[K     |████▍                           | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.6MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.3.1)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.21.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.17.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize) (0.14.0)\n","Installing collected packages: scikit-optimize\n","Successfully installed scikit-optimize-0.5.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2x8DHDnzGJMG","colab_type":"code","outputId":"31ded127-461a-49cc-be35-0cafc78b250b","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1572416479485,"user_tz":-330,"elapsed":1422,"user":{"displayName":"Kushagra Bhatia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8m2Hqe_J_VckJ1PlnRCIhfLS2zsKUH5E0SzgRfA=s64","userId":"12476490699397159076"}}},"source":["import skopt\n","from skopt import gbrt_minimize, gp_minimize\n","from skopt.utils import use_named_args\n","from skopt.space import Real, Categorical, Integer\n","from keras.optimizers import Adamax"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=DeprecationWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"H5reBG7yGnAC","colab_type":"code","colab":{}},"source":["dim_learning_rate = Real(low=1e-4, high=1e-2, prior='log-uniform',\n","                         name='learning_rate')\n","dim_no_epoch = Integer(low=20, high=80, name='no_epoch')\n","dim_embedding_size = Integer(low=16, high=256, name='embedding_size')\n","dim_lstm_output_size = Integer(low=16, high=256, name='lstm_output_size')\n","dim_pool_length =Integer(low=2,high=5,name='pool_length')\n","dim_batch_size = Integer(low=1, high=128, name='batch_size')\n","dim_adam_decay = Real(low=1e-6,high=1e-2,name=\"adam_decay\")\n","dim_no_filter = Integer(low=16,high=256,name='no_filter')\n","\n","dimensions = [dim_learning_rate,\n","              dim_batch_size,\n","              dim_adam_decay,\n","              dim_embedding_size,\n","              dim_lstm_output_size,\n","              dim_no_epoch,\n","              dim_no_filter,\n","             ]\n","default_parameters = [1e-3,32,1e-3, 128,128,50,128]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CGt78Ni96YK","colab_type":"code","colab":{}},"source":["@use_named_args(dimensions=dimensions)\n","def fitness(learning_rate,batch_size,adam_decay,embedding_size,lstm_output_size,no_epoch,no_filter):\n","    model = Sequential()\n","    model.add(Embedding(MAX_FEATURES, embedding_size, input_length=MAXLEN))\n","    model.add(Convolution1D(nb_filter=no_filter,filter_length=filter_length,border_mode='valid',activation='relu',subsample_length=1))\n","    model.add(MaxPooling1D(pool_length=pool_length))\n","    model.add(LSTM(lstm_output_size, dropout_W=0.2, dropout_U=0.2, return_sequences=True))\n","    model.add(LSTM(lstm_output_size, dropout_W=0.2, dropout_U=0.2, return_sequences=False))\n","    model.add(Dense(numclasses))\n","    model.add(Activation('softmax'))\n","    \n","    admax=Adamax(lr=learning_rate,decay=adam_decay)\n","    \n","    model.compile(loss='categorical_crossentropy',optimizer=admax,metrics=['accuracy'])\n","    \n","    X_train1,X_valid,y_train1,y_valid=train_test_split(X_train,y_train,test_size=0.2,random_state=42)\n","    bb=model.fit(X_train1, y_train1, batch_size=batch_size, shuffle=True, nb_epoch=no_epoch,validation_data=(X_valid, y_valid))\n","    \n","    accuracy=bb.history['val_acc'][-1]\n","    # Print the classification accuracy.\n","    print()\n","    print(\"Accuracy: {0:.2%}\".format(accuracy))\n","    print()\n","\n","\n","    # Delete the Keras model with these hyper-parameters from memory.\n","    del model\n","    \n","    K.clear_session()\n","    tf.reset_default_graph()\n","    \n","    return -accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"shGQLvmb9rbc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":144},"outputId":"30972385-3769-4e56-d9a6-d11f3aec1738","executionInfo":{"status":"ok","timestamp":1572416490107,"user_tz":-330,"elapsed":1025,"user":{"displayName":"Kushagra Bhatia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8m2Hqe_J_VckJ1PlnRCIhfLS2zsKUH5E0SzgRfA=s64","userId":"12476490699397159076"}}},"source":["K.clear_session()\n","tf.reset_default_graph()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BTTq3DNMJrA3","colab_type":"code","outputId":"478976d9-78bc-4f3d-ccde-d96e5bca32ad","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1572435352071,"user_tz":-330,"elapsed":6805459,"user":{"displayName":"Kushagra Bhatia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC8m2Hqe_J_VckJ1PlnRCIhfLS2zsKUH5E0SzgRfA=s64","userId":"12476490699397159076"}}},"source":["gp_result = gp_minimize(func=fitness,\n","                            dimensions=dimensions,\n","                            n_calls=12,\n","                            noise= 0.01,\n","                            n_jobs=-1,\n","                            kappa = 5,\n","                            x0=default_parameters)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Train on 2482 samples, validate on 621 samples\n","Epoch 1/50\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","2482/2482 [==============================] - 34s 14ms/step - loss: 0.9880 - acc: 0.5036 - val_loss: 0.9966 - val_acc: 0.4815\n","Epoch 2/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.9677 - acc: 0.5093 - val_loss: 0.9814 - val_acc: 0.4815\n","Epoch 3/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.9541 - acc: 0.5181 - val_loss: 0.9709 - val_acc: 0.4799\n","Epoch 4/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.9387 - acc: 0.5318 - val_loss: 0.9620 - val_acc: 0.4928\n","Epoch 5/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.9148 - acc: 0.5604 - val_loss: 0.9314 - val_acc: 0.5072\n","Epoch 6/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8897 - acc: 0.5761 - val_loss: 0.9077 - val_acc: 0.5668\n","Epoch 7/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8700 - acc: 0.5882 - val_loss: 0.8955 - val_acc: 0.5604\n","Epoch 8/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8735 - acc: 0.5890 - val_loss: 0.8986 - val_acc: 0.5700\n","Epoch 9/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8576 - acc: 0.6064 - val_loss: 0.9155 - val_acc: 0.5797\n","Epoch 10/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8463 - acc: 0.6124 - val_loss: 0.8776 - val_acc: 0.5926\n","Epoch 11/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8447 - acc: 0.6027 - val_loss: 0.8759 - val_acc: 0.5733\n","Epoch 12/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8380 - acc: 0.6112 - val_loss: 0.8688 - val_acc: 0.6006\n","Epoch 13/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8250 - acc: 0.6176 - val_loss: 0.8975 - val_acc: 0.5781\n","Epoch 14/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8169 - acc: 0.6189 - val_loss: 0.8787 - val_acc: 0.5797\n","Epoch 15/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8186 - acc: 0.6269 - val_loss: 0.8569 - val_acc: 0.5910\n","Epoch 16/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8055 - acc: 0.6394 - val_loss: 0.8558 - val_acc: 0.6023\n","Epoch 17/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8007 - acc: 0.6402 - val_loss: 0.8449 - val_acc: 0.6184\n","Epoch 18/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.8049 - acc: 0.6293 - val_loss: 0.8409 - val_acc: 0.6200\n","Epoch 19/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7906 - acc: 0.6394 - val_loss: 0.8490 - val_acc: 0.6216\n","Epoch 20/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7819 - acc: 0.6495 - val_loss: 0.8456 - val_acc: 0.6071\n","Epoch 21/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7837 - acc: 0.6463 - val_loss: 0.8711 - val_acc: 0.5797\n","Epoch 22/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7886 - acc: 0.6446 - val_loss: 0.8426 - val_acc: 0.6135\n","Epoch 23/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7789 - acc: 0.6495 - val_loss: 0.8351 - val_acc: 0.6071\n","Epoch 24/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7686 - acc: 0.6591 - val_loss: 0.8405 - val_acc: 0.6151\n","Epoch 25/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7679 - acc: 0.6612 - val_loss: 0.8351 - val_acc: 0.6232\n","Epoch 26/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7525 - acc: 0.6704 - val_loss: 0.8310 - val_acc: 0.6135\n","Epoch 27/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7601 - acc: 0.6648 - val_loss: 0.8286 - val_acc: 0.6248\n","Epoch 28/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7604 - acc: 0.6628 - val_loss: 0.8365 - val_acc: 0.6296\n","Epoch 29/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7447 - acc: 0.6716 - val_loss: 0.8338 - val_acc: 0.6119\n","Epoch 30/50\n","2482/2482 [==============================] - 24s 10ms/step - loss: 0.7414 - acc: 0.6732 - val_loss: 0.8315 - val_acc: 0.6377\n","Epoch 31/50\n","2482/2482 [==============================] - 24s 10ms/step - loss: 0.7440 - acc: 0.6704 - val_loss: 0.8218 - val_acc: 0.6409\n","Epoch 32/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7409 - acc: 0.6753 - val_loss: 0.8172 - val_acc: 0.6312\n","Epoch 33/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7417 - acc: 0.6732 - val_loss: 0.8247 - val_acc: 0.6312\n","Epoch 34/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7387 - acc: 0.6745 - val_loss: 0.8207 - val_acc: 0.6248\n","Epoch 35/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7305 - acc: 0.6853 - val_loss: 0.8148 - val_acc: 0.6377\n","Epoch 36/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7250 - acc: 0.6845 - val_loss: 0.8108 - val_acc: 0.6361\n","Epoch 37/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7287 - acc: 0.6761 - val_loss: 0.8184 - val_acc: 0.6264\n","Epoch 38/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7219 - acc: 0.6878 - val_loss: 0.8213 - val_acc: 0.6345\n","Epoch 39/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7279 - acc: 0.6737 - val_loss: 0.8047 - val_acc: 0.6409\n","Epoch 40/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7092 - acc: 0.6946 - val_loss: 0.8082 - val_acc: 0.6409\n","Epoch 41/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7166 - acc: 0.6837 - val_loss: 0.8131 - val_acc: 0.6490\n","Epoch 42/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7178 - acc: 0.6849 - val_loss: 0.8147 - val_acc: 0.6441\n","Epoch 43/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7122 - acc: 0.6886 - val_loss: 0.8067 - val_acc: 0.6393\n","Epoch 44/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7123 - acc: 0.6894 - val_loss: 0.8141 - val_acc: 0.6393\n","Epoch 45/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7054 - acc: 0.6918 - val_loss: 0.8030 - val_acc: 0.6425\n","Epoch 46/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.7078 - acc: 0.6938 - val_loss: 0.8068 - val_acc: 0.6393\n","Epoch 47/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.6959 - acc: 0.7039 - val_loss: 0.8218 - val_acc: 0.6425\n","Epoch 48/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.6982 - acc: 0.7043 - val_loss: 0.8074 - val_acc: 0.6473\n","Epoch 49/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.6880 - acc: 0.6966 - val_loss: 0.8115 - val_acc: 0.6377\n","Epoch 50/50\n","2482/2482 [==============================] - 25s 10ms/step - loss: 0.6856 - acc: 0.7055 - val_loss: 0.8079 - val_acc: 0.6425\n","\n","Accuracy: 64.25%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=17, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(41, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(41, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/70\n","2482/2482 [==============================] - 100s 40ms/step - loss: 0.9730 - acc: 0.5157 - val_loss: 0.9733 - val_acc: 0.4879\n","Epoch 2/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.9577 - acc: 0.5246 - val_loss: 0.9645 - val_acc: 0.4928\n","Epoch 3/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.9385 - acc: 0.5387 - val_loss: 0.9499 - val_acc: 0.4911\n","Epoch 4/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.9231 - acc: 0.5475 - val_loss: 0.9395 - val_acc: 0.5072\n","Epoch 5/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.9106 - acc: 0.5504 - val_loss: 0.9486 - val_acc: 0.5185\n","Epoch 6/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.8983 - acc: 0.5669 - val_loss: 0.9210 - val_acc: 0.5217\n","Epoch 7/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.8876 - acc: 0.5818 - val_loss: 0.9216 - val_acc: 0.5507\n","Epoch 8/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.8844 - acc: 0.5806 - val_loss: 0.9023 - val_acc: 0.5765\n","Epoch 9/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.8709 - acc: 0.6003 - val_loss: 0.8901 - val_acc: 0.5700\n","Epoch 10/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.8611 - acc: 0.6064 - val_loss: 0.8950 - val_acc: 0.5813\n","Epoch 11/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.8587 - acc: 0.6044 - val_loss: 0.8972 - val_acc: 0.5862\n","Epoch 12/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.8594 - acc: 0.5995 - val_loss: 0.8838 - val_acc: 0.5749\n","Epoch 13/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.8330 - acc: 0.6201 - val_loss: 0.8715 - val_acc: 0.5668\n","Epoch 14/70\n","2482/2482 [==============================] - 99s 40ms/step - loss: 0.8453 - acc: 0.6229 - val_loss: 0.8930 - val_acc: 0.5797\n","Epoch 15/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.8383 - acc: 0.6124 - val_loss: 0.8754 - val_acc: 0.5749\n","Epoch 16/70\n","2482/2482 [==============================] - 96s 39ms/step - loss: 0.8342 - acc: 0.6080 - val_loss: 0.8771 - val_acc: 0.5733\n","Epoch 17/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.8223 - acc: 0.6289 - val_loss: 0.8726 - val_acc: 0.5700\n","Epoch 18/70\n","2482/2482 [==============================] - 96s 39ms/step - loss: 0.8217 - acc: 0.6265 - val_loss: 0.8693 - val_acc: 0.5733\n","Epoch 19/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.8227 - acc: 0.6273 - val_loss: 0.8752 - val_acc: 0.5894\n","Epoch 20/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.8129 - acc: 0.6273 - val_loss: 0.8631 - val_acc: 0.5813\n","Epoch 21/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.8024 - acc: 0.6342 - val_loss: 0.8673 - val_acc: 0.5942\n","Epoch 22/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.8015 - acc: 0.6459 - val_loss: 0.8637 - val_acc: 0.5910\n","Epoch 23/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.8035 - acc: 0.6450 - val_loss: 0.8637 - val_acc: 0.5894\n","Epoch 24/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7940 - acc: 0.6410 - val_loss: 0.8644 - val_acc: 0.5958\n","Epoch 25/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7976 - acc: 0.6446 - val_loss: 0.8699 - val_acc: 0.5958\n","Epoch 26/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.8066 - acc: 0.6366 - val_loss: 0.8591 - val_acc: 0.5958\n","Epoch 27/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7978 - acc: 0.6559 - val_loss: 0.8555 - val_acc: 0.5894\n","Epoch 28/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7876 - acc: 0.6459 - val_loss: 0.8690 - val_acc: 0.6039\n","Epoch 29/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7969 - acc: 0.6434 - val_loss: 0.8622 - val_acc: 0.6055\n","Epoch 30/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7966 - acc: 0.6450 - val_loss: 0.8640 - val_acc: 0.6119\n","Epoch 31/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7782 - acc: 0.6624 - val_loss: 0.8646 - val_acc: 0.6055\n","Epoch 32/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7808 - acc: 0.6559 - val_loss: 0.8552 - val_acc: 0.5942\n","Epoch 33/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7887 - acc: 0.6563 - val_loss: 0.8645 - val_acc: 0.6023\n","Epoch 34/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7952 - acc: 0.6450 - val_loss: 0.8490 - val_acc: 0.6023\n","Epoch 35/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7802 - acc: 0.6499 - val_loss: 0.8504 - val_acc: 0.6039\n","Epoch 36/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7798 - acc: 0.6511 - val_loss: 0.8504 - val_acc: 0.6087\n","Epoch 37/70\n","2482/2482 [==============================] - 96s 39ms/step - loss: 0.7665 - acc: 0.6620 - val_loss: 0.8534 - val_acc: 0.5990\n","Epoch 38/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7693 - acc: 0.6595 - val_loss: 0.8576 - val_acc: 0.6119\n","Epoch 39/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.7687 - acc: 0.6600 - val_loss: 0.8511 - val_acc: 0.6039\n","Epoch 40/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.7756 - acc: 0.6583 - val_loss: 0.8574 - val_acc: 0.6071\n","Epoch 41/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.7700 - acc: 0.6604 - val_loss: 0.8558 - val_acc: 0.6023\n","Epoch 42/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.7640 - acc: 0.6708 - val_loss: 0.8535 - val_acc: 0.6103\n","Epoch 43/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7712 - acc: 0.6608 - val_loss: 0.8477 - val_acc: 0.6039\n","Epoch 44/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7647 - acc: 0.6644 - val_loss: 0.8529 - val_acc: 0.6039\n","Epoch 45/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7596 - acc: 0.6652 - val_loss: 0.8403 - val_acc: 0.6071\n","Epoch 46/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7646 - acc: 0.6692 - val_loss: 0.8400 - val_acc: 0.6039\n","Epoch 47/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7600 - acc: 0.6567 - val_loss: 0.8508 - val_acc: 0.6087\n","Epoch 48/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7486 - acc: 0.6757 - val_loss: 0.8403 - val_acc: 0.6103\n","Epoch 49/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7485 - acc: 0.6793 - val_loss: 0.8560 - val_acc: 0.6087\n","Epoch 50/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7468 - acc: 0.6672 - val_loss: 0.8455 - val_acc: 0.6135\n","Epoch 51/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7479 - acc: 0.6688 - val_loss: 0.8419 - val_acc: 0.6135\n","Epoch 52/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7666 - acc: 0.6587 - val_loss: 0.8405 - val_acc: 0.6119\n","Epoch 53/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7571 - acc: 0.6720 - val_loss: 0.8441 - val_acc: 0.6167\n","Epoch 54/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7547 - acc: 0.6789 - val_loss: 0.8391 - val_acc: 0.6103\n","Epoch 55/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7601 - acc: 0.6644 - val_loss: 0.8405 - val_acc: 0.6119\n","Epoch 56/70\n","2482/2482 [==============================] - 96s 39ms/step - loss: 0.7477 - acc: 0.6680 - val_loss: 0.8376 - val_acc: 0.6200\n","Epoch 57/70\n","2482/2482 [==============================] - 96s 39ms/step - loss: 0.7564 - acc: 0.6672 - val_loss: 0.8418 - val_acc: 0.6167\n","Epoch 58/70\n","2482/2482 [==============================] - 94s 38ms/step - loss: 0.7561 - acc: 0.6680 - val_loss: 0.8365 - val_acc: 0.6087\n","Epoch 59/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7558 - acc: 0.6664 - val_loss: 0.8375 - val_acc: 0.6119\n","Epoch 60/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7419 - acc: 0.6797 - val_loss: 0.8457 - val_acc: 0.6135\n","Epoch 61/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7417 - acc: 0.6704 - val_loss: 0.8458 - val_acc: 0.6200\n","Epoch 62/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7435 - acc: 0.6704 - val_loss: 0.8470 - val_acc: 0.6216\n","Epoch 63/70\n","2482/2482 [==============================] - 98s 39ms/step - loss: 0.7371 - acc: 0.6741 - val_loss: 0.8361 - val_acc: 0.6184\n","Epoch 64/70\n","2482/2482 [==============================] - 98s 40ms/step - loss: 0.7527 - acc: 0.6656 - val_loss: 0.8377 - val_acc: 0.6167\n","Epoch 65/70\n","2482/2482 [==============================] - 97s 39ms/step - loss: 0.7466 - acc: 0.6765 - val_loss: 0.8331 - val_acc: 0.6151\n","Epoch 66/70\n","2482/2482 [==============================] - 95s 38ms/step - loss: 0.7534 - acc: 0.6591 - val_loss: 0.8375 - val_acc: 0.6167\n","Epoch 67/70\n","2482/2482 [==============================] - 95s 38ms/step - loss: 0.7461 - acc: 0.6785 - val_loss: 0.8391 - val_acc: 0.6232\n","Epoch 68/70\n","2482/2482 [==============================] - 96s 39ms/step - loss: 0.7521 - acc: 0.6664 - val_loss: 0.8351 - val_acc: 0.6119\n","Epoch 69/70\n","2482/2482 [==============================] - 95s 38ms/step - loss: 0.7501 - acc: 0.6712 - val_loss: 0.8388 - val_acc: 0.6216\n","Epoch 70/70\n","2482/2482 [==============================] - 96s 39ms/step - loss: 0.7382 - acc: 0.6765 - val_loss: 0.8342 - val_acc: 0.6087\n","\n","Accuracy: 60.87%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=77, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(184, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(184, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/31\n","2482/2482 [==============================] - 16s 6ms/step - loss: 0.9914 - acc: 0.5077 - val_loss: 0.9938 - val_acc: 0.4815\n","Epoch 2/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9728 - acc: 0.5089 - val_loss: 0.9886 - val_acc: 0.4815\n","Epoch 3/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9605 - acc: 0.5097 - val_loss: 0.9900 - val_acc: 0.4815\n","Epoch 4/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9569 - acc: 0.5222 - val_loss: 0.9826 - val_acc: 0.4895\n","Epoch 5/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9528 - acc: 0.5318 - val_loss: 0.9752 - val_acc: 0.4847\n","Epoch 6/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9425 - acc: 0.5383 - val_loss: 0.9707 - val_acc: 0.4928\n","Epoch 7/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9390 - acc: 0.5326 - val_loss: 0.9696 - val_acc: 0.4928\n","Epoch 8/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9373 - acc: 0.5355 - val_loss: 0.9592 - val_acc: 0.4815\n","Epoch 9/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9299 - acc: 0.5407 - val_loss: 0.9558 - val_acc: 0.4911\n","Epoch 10/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9250 - acc: 0.5415 - val_loss: 0.9520 - val_acc: 0.4944\n","Epoch 11/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9161 - acc: 0.5608 - val_loss: 0.9446 - val_acc: 0.5105\n","Epoch 12/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9082 - acc: 0.5637 - val_loss: 0.9356 - val_acc: 0.5137\n","Epoch 13/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9015 - acc: 0.5633 - val_loss: 0.9260 - val_acc: 0.5217\n","Epoch 14/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.9020 - acc: 0.5713 - val_loss: 0.9219 - val_acc: 0.5314\n","Epoch 15/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8967 - acc: 0.5733 - val_loss: 0.9149 - val_acc: 0.5491\n","Epoch 16/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8957 - acc: 0.5665 - val_loss: 0.9138 - val_acc: 0.5427\n","Epoch 17/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8897 - acc: 0.5802 - val_loss: 0.9129 - val_acc: 0.5443\n","Epoch 18/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8841 - acc: 0.5818 - val_loss: 0.9132 - val_acc: 0.5507\n","Epoch 19/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8823 - acc: 0.5814 - val_loss: 0.9037 - val_acc: 0.5588\n","Epoch 20/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8766 - acc: 0.5907 - val_loss: 0.9140 - val_acc: 0.5523\n","Epoch 21/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8866 - acc: 0.5741 - val_loss: 0.9077 - val_acc: 0.5507\n","Epoch 22/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8829 - acc: 0.5874 - val_loss: 0.8967 - val_acc: 0.5668\n","Epoch 23/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8827 - acc: 0.5798 - val_loss: 0.8975 - val_acc: 0.5668\n","Epoch 24/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8775 - acc: 0.5862 - val_loss: 0.9032 - val_acc: 0.5507\n","Epoch 25/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8698 - acc: 0.5902 - val_loss: 0.8952 - val_acc: 0.5684\n","Epoch 26/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8721 - acc: 0.5830 - val_loss: 0.8941 - val_acc: 0.5572\n","Epoch 27/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8660 - acc: 0.5923 - val_loss: 0.8964 - val_acc: 0.5700\n","Epoch 28/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8703 - acc: 0.5951 - val_loss: 0.8940 - val_acc: 0.5668\n","Epoch 29/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8692 - acc: 0.5955 - val_loss: 0.8901 - val_acc: 0.5652\n","Epoch 30/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8697 - acc: 0.5858 - val_loss: 0.8924 - val_acc: 0.5700\n","Epoch 31/31\n","2482/2482 [==============================] - 13s 5ms/step - loss: 0.8665 - acc: 0.5979 - val_loss: 0.8946 - val_acc: 0.5684\n","\n","Accuracy: 56.84%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=146, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(249, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(249, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/56\n","2482/2482 [==============================] - 17s 7ms/step - loss: 0.9859 - acc: 0.5060 - val_loss: 0.9900 - val_acc: 0.4815\n","Epoch 2/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.9558 - acc: 0.5165 - val_loss: 0.9866 - val_acc: 0.4831\n","Epoch 3/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.9442 - acc: 0.5290 - val_loss: 0.9549 - val_acc: 0.4944\n","Epoch 4/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.9160 - acc: 0.5508 - val_loss: 0.9218 - val_acc: 0.5314\n","Epoch 5/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8879 - acc: 0.5745 - val_loss: 0.9123 - val_acc: 0.5427\n","Epoch 6/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8735 - acc: 0.5830 - val_loss: 0.8999 - val_acc: 0.5604\n","Epoch 7/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8612 - acc: 0.5886 - val_loss: 0.8916 - val_acc: 0.5684\n","Epoch 8/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8419 - acc: 0.6072 - val_loss: 0.8771 - val_acc: 0.5717\n","Epoch 9/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8355 - acc: 0.6128 - val_loss: 0.8809 - val_acc: 0.5668\n","Epoch 10/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8252 - acc: 0.6253 - val_loss: 0.8739 - val_acc: 0.5781\n","Epoch 11/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8181 - acc: 0.6120 - val_loss: 0.8817 - val_acc: 0.5781\n","Epoch 12/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8143 - acc: 0.6265 - val_loss: 0.8690 - val_acc: 0.5813\n","Epoch 13/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8102 - acc: 0.6317 - val_loss: 0.8594 - val_acc: 0.5862\n","Epoch 14/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7993 - acc: 0.6418 - val_loss: 0.8609 - val_acc: 0.5878\n","Epoch 15/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8045 - acc: 0.6309 - val_loss: 0.8493 - val_acc: 0.5974\n","Epoch 16/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.8013 - acc: 0.6326 - val_loss: 0.8491 - val_acc: 0.5894\n","Epoch 17/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7886 - acc: 0.6442 - val_loss: 0.8586 - val_acc: 0.5878\n","Epoch 18/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7856 - acc: 0.6467 - val_loss: 0.8440 - val_acc: 0.5894\n","Epoch 19/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7842 - acc: 0.6515 - val_loss: 0.8351 - val_acc: 0.6087\n","Epoch 20/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7676 - acc: 0.6531 - val_loss: 0.8388 - val_acc: 0.5974\n","Epoch 21/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7650 - acc: 0.6583 - val_loss: 0.8358 - val_acc: 0.6006\n","Epoch 22/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7560 - acc: 0.6547 - val_loss: 0.8436 - val_acc: 0.6135\n","Epoch 23/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7580 - acc: 0.6656 - val_loss: 0.8499 - val_acc: 0.6023\n","Epoch 24/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7513 - acc: 0.6632 - val_loss: 0.8447 - val_acc: 0.5910\n","Epoch 25/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7520 - acc: 0.6644 - val_loss: 0.8316 - val_acc: 0.6119\n","Epoch 26/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7428 - acc: 0.6660 - val_loss: 0.8367 - val_acc: 0.6103\n","Epoch 27/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7442 - acc: 0.6676 - val_loss: 0.8219 - val_acc: 0.6103\n","Epoch 28/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7423 - acc: 0.6765 - val_loss: 0.8255 - val_acc: 0.6119\n","Epoch 29/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7417 - acc: 0.6737 - val_loss: 0.8225 - val_acc: 0.6071\n","Epoch 30/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7411 - acc: 0.6688 - val_loss: 0.8291 - val_acc: 0.6167\n","Epoch 31/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7338 - acc: 0.6801 - val_loss: 0.8248 - val_acc: 0.6119\n","Epoch 32/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7354 - acc: 0.6785 - val_loss: 0.8327 - val_acc: 0.6071\n","Epoch 33/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7260 - acc: 0.6789 - val_loss: 0.8300 - val_acc: 0.6167\n","Epoch 34/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7320 - acc: 0.6773 - val_loss: 0.8371 - val_acc: 0.6119\n","Epoch 35/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7228 - acc: 0.6765 - val_loss: 0.8340 - val_acc: 0.6055\n","Epoch 36/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7318 - acc: 0.6829 - val_loss: 0.8245 - val_acc: 0.6167\n","Epoch 37/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7112 - acc: 0.6882 - val_loss: 0.8165 - val_acc: 0.6103\n","Epoch 38/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7165 - acc: 0.6829 - val_loss: 0.8237 - val_acc: 0.6151\n","Epoch 39/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7215 - acc: 0.6902 - val_loss: 0.8186 - val_acc: 0.6200\n","Epoch 40/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7188 - acc: 0.6902 - val_loss: 0.8148 - val_acc: 0.6216\n","Epoch 41/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7144 - acc: 0.6801 - val_loss: 0.8294 - val_acc: 0.6248\n","Epoch 42/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7104 - acc: 0.6914 - val_loss: 0.8180 - val_acc: 0.6167\n","Epoch 43/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7105 - acc: 0.6950 - val_loss: 0.8190 - val_acc: 0.6216\n","Epoch 44/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7170 - acc: 0.6749 - val_loss: 0.8123 - val_acc: 0.6200\n","Epoch 45/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7111 - acc: 0.6906 - val_loss: 0.8139 - val_acc: 0.6216\n","Epoch 46/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6972 - acc: 0.6958 - val_loss: 0.8206 - val_acc: 0.6248\n","Epoch 47/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7030 - acc: 0.6926 - val_loss: 0.8176 - val_acc: 0.6232\n","Epoch 48/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.7029 - acc: 0.7043 - val_loss: 0.8142 - val_acc: 0.6216\n","Epoch 49/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6884 - acc: 0.7055 - val_loss: 0.8132 - val_acc: 0.6280\n","Epoch 50/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6935 - acc: 0.6994 - val_loss: 0.8114 - val_acc: 0.6248\n","Epoch 51/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6900 - acc: 0.7002 - val_loss: 0.8195 - val_acc: 0.6232\n","Epoch 52/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6876 - acc: 0.6974 - val_loss: 0.8118 - val_acc: 0.6200\n","Epoch 53/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6838 - acc: 0.7023 - val_loss: 0.8178 - val_acc: 0.6280\n","Epoch 54/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6973 - acc: 0.6950 - val_loss: 0.8240 - val_acc: 0.6216\n","Epoch 55/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6961 - acc: 0.6958 - val_loss: 0.8177 - val_acc: 0.6264\n","Epoch 56/56\n","2482/2482 [==============================] - 15s 6ms/step - loss: 0.6810 - acc: 0.6966 - val_loss: 0.8117 - val_acc: 0.6184\n","\n","Accuracy: 61.84%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=229, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(130, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(130, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/34\n","2482/2482 [==============================] - 44s 18ms/step - loss: 0.9743 - acc: 0.5000 - val_loss: 0.9592 - val_acc: 0.5089\n","Epoch 2/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.9163 - acc: 0.5560 - val_loss: 0.9276 - val_acc: 0.5330\n","Epoch 3/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.8791 - acc: 0.5850 - val_loss: 0.8879 - val_acc: 0.5797\n","Epoch 4/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.8351 - acc: 0.6213 - val_loss: 0.8435 - val_acc: 0.6023\n","Epoch 5/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.8010 - acc: 0.6394 - val_loss: 0.8280 - val_acc: 0.6151\n","Epoch 6/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.7655 - acc: 0.6547 - val_loss: 0.8373 - val_acc: 0.6232\n","Epoch 7/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.7383 - acc: 0.6745 - val_loss: 0.8523 - val_acc: 0.6377\n","Epoch 8/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.6989 - acc: 0.6998 - val_loss: 0.8117 - val_acc: 0.6490\n","Epoch 9/34\n","2482/2482 [==============================] - 42s 17ms/step - loss: 0.6681 - acc: 0.7232 - val_loss: 0.8477 - val_acc: 0.6490\n","Epoch 10/34\n","2482/2482 [==============================] - 42s 17ms/step - loss: 0.6445 - acc: 0.7244 - val_loss: 0.7878 - val_acc: 0.6651\n","Epoch 11/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.6144 - acc: 0.7450 - val_loss: 0.7984 - val_acc: 0.6457\n","Epoch 12/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.5846 - acc: 0.7639 - val_loss: 0.8061 - val_acc: 0.6570\n","Epoch 13/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.5619 - acc: 0.7687 - val_loss: 0.9010 - val_acc: 0.6280\n","Epoch 14/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.5665 - acc: 0.7623 - val_loss: 0.8383 - val_acc: 0.6409\n","Epoch 15/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.5414 - acc: 0.7687 - val_loss: 0.8965 - val_acc: 0.6393\n","Epoch 16/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.5122 - acc: 0.7921 - val_loss: 0.8857 - val_acc: 0.6473\n","Epoch 17/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.4940 - acc: 0.8110 - val_loss: 0.9307 - val_acc: 0.6393\n","Epoch 18/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.4519 - acc: 0.8219 - val_loss: 0.8964 - val_acc: 0.6618\n","Epoch 19/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.4621 - acc: 0.8106 - val_loss: 0.9407 - val_acc: 0.6538\n","Epoch 20/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.4373 - acc: 0.8268 - val_loss: 0.9210 - val_acc: 0.6570\n","Epoch 21/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.4104 - acc: 0.8425 - val_loss: 0.9829 - val_acc: 0.6151\n","Epoch 22/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.4042 - acc: 0.8372 - val_loss: 1.0006 - val_acc: 0.6473\n","Epoch 23/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.3951 - acc: 0.8405 - val_loss: 0.9599 - val_acc: 0.6345\n","Epoch 24/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.3655 - acc: 0.8574 - val_loss: 1.0939 - val_acc: 0.6377\n","Epoch 25/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.3612 - acc: 0.8521 - val_loss: 1.0568 - val_acc: 0.6441\n","Epoch 26/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.3137 - acc: 0.8820 - val_loss: 1.1398 - val_acc: 0.6490\n","Epoch 27/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.3247 - acc: 0.8707 - val_loss: 1.1422 - val_acc: 0.6345\n","Epoch 28/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.3073 - acc: 0.8820 - val_loss: 1.1781 - val_acc: 0.6441\n","Epoch 29/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.2959 - acc: 0.8799 - val_loss: 1.1998 - val_acc: 0.6312\n","Epoch 30/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.2895 - acc: 0.8884 - val_loss: 1.2557 - val_acc: 0.6312\n","Epoch 31/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.2789 - acc: 0.8880 - val_loss: 1.2269 - val_acc: 0.6506\n","Epoch 32/34\n","2482/2482 [==============================] - 41s 16ms/step - loss: 0.2574 - acc: 0.8961 - val_loss: 1.2528 - val_acc: 0.6618\n","Epoch 33/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.2505 - acc: 0.9033 - val_loss: 1.3398 - val_acc: 0.6554\n","Epoch 34/34\n","2482/2482 [==============================] - 41s 17ms/step - loss: 0.2400 - acc: 0.9085 - val_loss: 1.3459 - val_acc: 0.6586\n","\n","Accuracy: 65.86%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=54, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(250, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(250, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/77\n","2482/2482 [==============================] - 48s 20ms/step - loss: 0.9819 - acc: 0.5121 - val_loss: 0.9577 - val_acc: 0.5137\n","Epoch 2/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9180 - acc: 0.5641 - val_loss: 0.9058 - val_acc: 0.5556\n","Epoch 3/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.8984 - acc: 0.5757 - val_loss: 0.9241 - val_acc: 0.5572\n","Epoch 4/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.8607 - acc: 0.5959 - val_loss: 0.8764 - val_acc: 0.5829\n","Epoch 5/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.8435 - acc: 0.6156 - val_loss: 0.8591 - val_acc: 0.5829\n","Epoch 6/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.8079 - acc: 0.6362 - val_loss: 0.8607 - val_acc: 0.5958\n","Epoch 7/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.7863 - acc: 0.6422 - val_loss: 0.8506 - val_acc: 0.6023\n","Epoch 8/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.7795 - acc: 0.6495 - val_loss: 0.8378 - val_acc: 0.5974\n","Epoch 9/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.7750 - acc: 0.6583 - val_loss: 0.8425 - val_acc: 0.6184\n","Epoch 10/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.7583 - acc: 0.6628 - val_loss: 0.8606 - val_acc: 0.5958\n","Epoch 11/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.7529 - acc: 0.6644 - val_loss: 0.8291 - val_acc: 0.6200\n","Epoch 12/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.7356 - acc: 0.6688 - val_loss: 0.8225 - val_acc: 0.6167\n","Epoch 13/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.7213 - acc: 0.6906 - val_loss: 0.8099 - val_acc: 0.6296\n","Epoch 14/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.7178 - acc: 0.6886 - val_loss: 0.8217 - val_acc: 0.6312\n","Epoch 15/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.7045 - acc: 0.6938 - val_loss: 0.7934 - val_acc: 0.6312\n","Epoch 16/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.6961 - acc: 0.6998 - val_loss: 0.7957 - val_acc: 0.6409\n","Epoch 17/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.6944 - acc: 0.6914 - val_loss: 0.8065 - val_acc: 0.6216\n","Epoch 18/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.6768 - acc: 0.7095 - val_loss: 0.8025 - val_acc: 0.6345\n","Epoch 19/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6908 - acc: 0.6970 - val_loss: 0.7995 - val_acc: 0.6280\n","Epoch 20/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6742 - acc: 0.7055 - val_loss: 0.7923 - val_acc: 0.6361\n","Epoch 21/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6633 - acc: 0.7143 - val_loss: 0.8029 - val_acc: 0.6312\n","Epoch 22/77\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.6728 - acc: 0.7103 - val_loss: 0.8034 - val_acc: 0.6312\n","Epoch 23/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6713 - acc: 0.7151 - val_loss: 0.7853 - val_acc: 0.6506\n","Epoch 24/77\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.6579 - acc: 0.7184 - val_loss: 0.7998 - val_acc: 0.6329\n","Epoch 25/77\n","2482/2482 [==============================] - 45s 18ms/step - loss: 0.6419 - acc: 0.7357 - val_loss: 0.7888 - val_acc: 0.6409\n","Epoch 26/77\n","2482/2482 [==============================] - 45s 18ms/step - loss: 0.6422 - acc: 0.7212 - val_loss: 0.8202 - val_acc: 0.6345\n","Epoch 27/77\n","2482/2482 [==============================] - 45s 18ms/step - loss: 0.6507 - acc: 0.7156 - val_loss: 0.7819 - val_acc: 0.6522\n","Epoch 28/77\n","2482/2482 [==============================] - 45s 18ms/step - loss: 0.6417 - acc: 0.7236 - val_loss: 0.7958 - val_acc: 0.6554\n","Epoch 29/77\n","2482/2482 [==============================] - 52s 21ms/step - loss: 0.6395 - acc: 0.7337 - val_loss: 0.8064 - val_acc: 0.6345\n","Epoch 30/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6409 - acc: 0.7280 - val_loss: 0.7896 - val_acc: 0.6425\n","Epoch 31/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.6284 - acc: 0.7244 - val_loss: 0.8120 - val_acc: 0.6296\n","Epoch 32/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6323 - acc: 0.7284 - val_loss: 0.7980 - val_acc: 0.6490\n","Epoch 33/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6245 - acc: 0.7264 - val_loss: 0.7888 - val_acc: 0.6490\n","Epoch 34/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6276 - acc: 0.7252 - val_loss: 0.7863 - val_acc: 0.6473\n","Epoch 35/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6238 - acc: 0.7288 - val_loss: 0.7918 - val_acc: 0.6457\n","Epoch 36/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.6183 - acc: 0.7293 - val_loss: 0.7984 - val_acc: 0.6457\n","Epoch 37/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.6207 - acc: 0.7421 - val_loss: 0.8041 - val_acc: 0.6409\n","Epoch 38/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.6117 - acc: 0.7474 - val_loss: 0.7980 - val_acc: 0.6425\n","Epoch 39/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.5985 - acc: 0.7458 - val_loss: 0.8079 - val_acc: 0.6457\n","Epoch 40/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.6040 - acc: 0.7438 - val_loss: 0.7990 - val_acc: 0.6473\n","Epoch 41/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.5875 - acc: 0.7550 - val_loss: 0.8119 - val_acc: 0.6409\n","Epoch 42/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5949 - acc: 0.7434 - val_loss: 0.8118 - val_acc: 0.6393\n","Epoch 43/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5976 - acc: 0.7546 - val_loss: 0.8069 - val_acc: 0.6457\n","Epoch 44/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.6044 - acc: 0.7498 - val_loss: 0.8002 - val_acc: 0.6425\n","Epoch 45/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5835 - acc: 0.7546 - val_loss: 0.8163 - val_acc: 0.6361\n","Epoch 46/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5703 - acc: 0.7587 - val_loss: 0.8160 - val_acc: 0.6409\n","Epoch 47/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5899 - acc: 0.7554 - val_loss: 0.8241 - val_acc: 0.6393\n","Epoch 48/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5770 - acc: 0.7554 - val_loss: 0.8042 - val_acc: 0.6457\n","Epoch 49/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5877 - acc: 0.7611 - val_loss: 0.8164 - val_acc: 0.6457\n","Epoch 50/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5793 - acc: 0.7579 - val_loss: 0.8071 - val_acc: 0.6457\n","Epoch 51/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5806 - acc: 0.7450 - val_loss: 0.8140 - val_acc: 0.6473\n","Epoch 52/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5787 - acc: 0.7526 - val_loss: 0.8194 - val_acc: 0.6441\n","Epoch 53/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.5775 - acc: 0.7546 - val_loss: 0.8142 - val_acc: 0.6506\n","Epoch 54/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5814 - acc: 0.7587 - val_loss: 0.8133 - val_acc: 0.6377\n","Epoch 55/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5772 - acc: 0.7663 - val_loss: 0.8095 - val_acc: 0.6393\n","Epoch 56/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5584 - acc: 0.7571 - val_loss: 0.8139 - val_acc: 0.6506\n","Epoch 57/77\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.5649 - acc: 0.7603 - val_loss: 0.8305 - val_acc: 0.6377\n","Epoch 58/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5652 - acc: 0.7699 - val_loss: 0.8190 - val_acc: 0.6409\n","Epoch 59/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5746 - acc: 0.7599 - val_loss: 0.8253 - val_acc: 0.6457\n","Epoch 60/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5624 - acc: 0.7720 - val_loss: 0.8293 - val_acc: 0.6409\n","Epoch 61/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5512 - acc: 0.7651 - val_loss: 0.8416 - val_acc: 0.6457\n","Epoch 62/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5657 - acc: 0.7623 - val_loss: 0.8276 - val_acc: 0.6441\n","Epoch 63/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5573 - acc: 0.7627 - val_loss: 0.8334 - val_acc: 0.6409\n","Epoch 64/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5536 - acc: 0.7583 - val_loss: 0.8314 - val_acc: 0.6457\n","Epoch 65/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.5535 - acc: 0.7707 - val_loss: 0.8329 - val_acc: 0.6441\n","Epoch 66/77\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.5667 - acc: 0.7571 - val_loss: 0.8317 - val_acc: 0.6457\n","Epoch 67/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.5571 - acc: 0.7599 - val_loss: 0.8218 - val_acc: 0.6441\n","Epoch 68/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5439 - acc: 0.7691 - val_loss: 0.8229 - val_acc: 0.6425\n","Epoch 69/77\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.5409 - acc: 0.7744 - val_loss: 0.8383 - val_acc: 0.6457\n","Epoch 70/77\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.5477 - acc: 0.7699 - val_loss: 0.8431 - val_acc: 0.6441\n","Epoch 71/77\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.5608 - acc: 0.7599 - val_loss: 0.8241 - val_acc: 0.6409\n","Epoch 72/77\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.5476 - acc: 0.7712 - val_loss: 0.8382 - val_acc: 0.6441\n","Epoch 73/77\n","2482/2482 [==============================] - 45s 18ms/step - loss: 0.5604 - acc: 0.7571 - val_loss: 0.8293 - val_acc: 0.6425\n","Epoch 74/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5331 - acc: 0.7768 - val_loss: 0.8389 - val_acc: 0.6393\n","Epoch 75/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5373 - acc: 0.7659 - val_loss: 0.8394 - val_acc: 0.6361\n","Epoch 76/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5352 - acc: 0.7707 - val_loss: 0.8297 - val_acc: 0.6312\n","Epoch 77/77\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.5376 - acc: 0.7824 - val_loss: 0.8204 - val_acc: 0.6329\n","\n","Accuracy: 63.29%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=122, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(244, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(244, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/47\n","2482/2482 [==============================] - 11s 5ms/step - loss: 1.0237 - acc: 0.4823 - val_loss: 0.9942 - val_acc: 0.4815\n","Epoch 2/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9717 - acc: 0.4847 - val_loss: 0.9804 - val_acc: 0.4831\n","Epoch 3/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9479 - acc: 0.5342 - val_loss: 0.9463 - val_acc: 0.5266\n","Epoch 4/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9138 - acc: 0.5737 - val_loss: 0.9309 - val_acc: 0.5411\n","Epoch 5/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8883 - acc: 0.5745 - val_loss: 0.9006 - val_acc: 0.5556\n","Epoch 6/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8641 - acc: 0.5995 - val_loss: 0.8914 - val_acc: 0.5572\n","Epoch 7/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8501 - acc: 0.6052 - val_loss: 0.9015 - val_acc: 0.5894\n","Epoch 8/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8427 - acc: 0.6100 - val_loss: 0.8746 - val_acc: 0.5781\n","Epoch 9/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8271 - acc: 0.6193 - val_loss: 0.8948 - val_acc: 0.5684\n","Epoch 10/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8210 - acc: 0.6189 - val_loss: 0.9259 - val_acc: 0.5733\n","Epoch 11/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8116 - acc: 0.6297 - val_loss: 0.8454 - val_acc: 0.5910\n","Epoch 12/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7934 - acc: 0.6463 - val_loss: 0.8459 - val_acc: 0.5942\n","Epoch 13/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7777 - acc: 0.6471 - val_loss: 0.8658 - val_acc: 0.5974\n","Epoch 14/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7931 - acc: 0.6507 - val_loss: 0.8309 - val_acc: 0.6055\n","Epoch 15/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7672 - acc: 0.6559 - val_loss: 0.8508 - val_acc: 0.5958\n","Epoch 16/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7508 - acc: 0.6644 - val_loss: 0.8461 - val_acc: 0.6006\n","Epoch 17/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7353 - acc: 0.6849 - val_loss: 0.8203 - val_acc: 0.6087\n","Epoch 18/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7368 - acc: 0.6821 - val_loss: 0.8185 - val_acc: 0.6039\n","Epoch 19/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7382 - acc: 0.6712 - val_loss: 0.8076 - val_acc: 0.6151\n","Epoch 20/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7226 - acc: 0.6841 - val_loss: 0.8160 - val_acc: 0.6232\n","Epoch 21/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7141 - acc: 0.6890 - val_loss: 0.8223 - val_acc: 0.6377\n","Epoch 22/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7142 - acc: 0.6910 - val_loss: 0.8148 - val_acc: 0.6264\n","Epoch 23/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7038 - acc: 0.6886 - val_loss: 0.8297 - val_acc: 0.6119\n","Epoch 24/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6992 - acc: 0.6902 - val_loss: 0.8152 - val_acc: 0.6167\n","Epoch 25/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6911 - acc: 0.6994 - val_loss: 0.8101 - val_acc: 0.6216\n","Epoch 26/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6961 - acc: 0.6946 - val_loss: 0.8180 - val_acc: 0.6167\n","Epoch 27/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6768 - acc: 0.7063 - val_loss: 0.8091 - val_acc: 0.6345\n","Epoch 28/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6698 - acc: 0.7115 - val_loss: 0.8147 - val_acc: 0.6264\n","Epoch 29/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6795 - acc: 0.7091 - val_loss: 0.8037 - val_acc: 0.6280\n","Epoch 30/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6724 - acc: 0.7176 - val_loss: 0.8119 - val_acc: 0.6377\n","Epoch 31/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6607 - acc: 0.7164 - val_loss: 0.8087 - val_acc: 0.6441\n","Epoch 32/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6630 - acc: 0.7188 - val_loss: 0.8132 - val_acc: 0.6329\n","Epoch 33/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6593 - acc: 0.7172 - val_loss: 0.8117 - val_acc: 0.6345\n","Epoch 34/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6478 - acc: 0.7284 - val_loss: 0.8029 - val_acc: 0.6490\n","Epoch 35/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6459 - acc: 0.7220 - val_loss: 0.8063 - val_acc: 0.6312\n","Epoch 36/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6457 - acc: 0.7240 - val_loss: 0.8130 - val_acc: 0.6361\n","Epoch 37/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6392 - acc: 0.7353 - val_loss: 0.8158 - val_acc: 0.6361\n","Epoch 38/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6538 - acc: 0.7208 - val_loss: 0.8019 - val_acc: 0.6329\n","Epoch 39/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6548 - acc: 0.7244 - val_loss: 0.8177 - val_acc: 0.6441\n","Epoch 40/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6359 - acc: 0.7405 - val_loss: 0.8034 - val_acc: 0.6377\n","Epoch 41/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6271 - acc: 0.7365 - val_loss: 0.8025 - val_acc: 0.6457\n","Epoch 42/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6277 - acc: 0.7341 - val_loss: 0.8201 - val_acc: 0.6457\n","Epoch 43/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6405 - acc: 0.7224 - val_loss: 0.7994 - val_acc: 0.6490\n","Epoch 44/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6299 - acc: 0.7369 - val_loss: 0.8002 - val_acc: 0.6393\n","Epoch 45/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6166 - acc: 0.7385 - val_loss: 0.8149 - val_acc: 0.6377\n","Epoch 46/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6193 - acc: 0.7434 - val_loss: 0.8023 - val_acc: 0.6425\n","Epoch 47/47\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.6274 - acc: 0.7325 - val_loss: 0.8175 - val_acc: 0.6473\n","\n","Accuracy: 64.73%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=50, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(44, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(44, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/70\n","2482/2482 [==============================] - 12s 5ms/step - loss: 0.9803 - acc: 0.5077 - val_loss: 0.9848 - val_acc: 0.4815\n","Epoch 2/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.9584 - acc: 0.5185 - val_loss: 0.9812 - val_acc: 0.4960\n","Epoch 3/70\n","2482/2482 [==============================] - 11s 4ms/step - loss: 0.9441 - acc: 0.5230 - val_loss: 0.9557 - val_acc: 0.4960\n","Epoch 4/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.9248 - acc: 0.5399 - val_loss: 0.9256 - val_acc: 0.5250\n","Epoch 5/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8941 - acc: 0.5717 - val_loss: 0.9053 - val_acc: 0.5556\n","Epoch 6/70\n","2482/2482 [==============================] - 11s 4ms/step - loss: 0.8761 - acc: 0.5774 - val_loss: 0.8910 - val_acc: 0.5652\n","Epoch 7/70\n","2482/2482 [==============================] - 11s 4ms/step - loss: 0.8642 - acc: 0.5951 - val_loss: 0.8939 - val_acc: 0.5459\n","Epoch 8/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8628 - acc: 0.5886 - val_loss: 0.8875 - val_acc: 0.5700\n","Epoch 9/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8484 - acc: 0.6096 - val_loss: 0.8869 - val_acc: 0.5797\n","Epoch 10/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8477 - acc: 0.6096 - val_loss: 0.8788 - val_acc: 0.5926\n","Epoch 11/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8375 - acc: 0.6112 - val_loss: 0.8607 - val_acc: 0.5894\n","Epoch 12/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8278 - acc: 0.6245 - val_loss: 0.8527 - val_acc: 0.5926\n","Epoch 13/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8203 - acc: 0.6277 - val_loss: 0.8590 - val_acc: 0.5910\n","Epoch 14/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8223 - acc: 0.6189 - val_loss: 0.8578 - val_acc: 0.5942\n","Epoch 15/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8151 - acc: 0.6366 - val_loss: 0.8500 - val_acc: 0.5974\n","Epoch 16/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.8127 - acc: 0.6301 - val_loss: 0.8442 - val_acc: 0.6023\n","Epoch 17/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7954 - acc: 0.6442 - val_loss: 0.8532 - val_acc: 0.5910\n","Epoch 18/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7995 - acc: 0.6450 - val_loss: 0.8517 - val_acc: 0.5926\n","Epoch 19/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7864 - acc: 0.6543 - val_loss: 0.8361 - val_acc: 0.6087\n","Epoch 20/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7872 - acc: 0.6467 - val_loss: 0.8409 - val_acc: 0.6087\n","Epoch 21/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7860 - acc: 0.6555 - val_loss: 0.8357 - val_acc: 0.6103\n","Epoch 22/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7861 - acc: 0.6519 - val_loss: 0.8360 - val_acc: 0.6103\n","Epoch 23/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7767 - acc: 0.6575 - val_loss: 0.8328 - val_acc: 0.6119\n","Epoch 24/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7763 - acc: 0.6632 - val_loss: 0.8379 - val_acc: 0.6200\n","Epoch 25/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7753 - acc: 0.6555 - val_loss: 0.8338 - val_acc: 0.6135\n","Epoch 26/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7688 - acc: 0.6511 - val_loss: 0.8293 - val_acc: 0.6232\n","Epoch 27/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7612 - acc: 0.6591 - val_loss: 0.8338 - val_acc: 0.6119\n","Epoch 28/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7564 - acc: 0.6620 - val_loss: 0.8260 - val_acc: 0.6264\n","Epoch 29/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7537 - acc: 0.6636 - val_loss: 0.8389 - val_acc: 0.6264\n","Epoch 30/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7615 - acc: 0.6684 - val_loss: 0.8295 - val_acc: 0.6264\n","Epoch 31/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7466 - acc: 0.6797 - val_loss: 0.8208 - val_acc: 0.6280\n","Epoch 32/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7471 - acc: 0.6704 - val_loss: 0.8254 - val_acc: 0.6216\n","Epoch 33/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7483 - acc: 0.6732 - val_loss: 0.8176 - val_acc: 0.6264\n","Epoch 34/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7458 - acc: 0.6668 - val_loss: 0.8280 - val_acc: 0.6232\n","Epoch 35/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7653 - acc: 0.6728 - val_loss: 0.8181 - val_acc: 0.6216\n","Epoch 36/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7395 - acc: 0.6761 - val_loss: 0.8369 - val_acc: 0.6184\n","Epoch 37/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7358 - acc: 0.6777 - val_loss: 0.8301 - val_acc: 0.6248\n","Epoch 38/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7472 - acc: 0.6749 - val_loss: 0.8325 - val_acc: 0.6248\n","Epoch 39/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7324 - acc: 0.6857 - val_loss: 0.8165 - val_acc: 0.6280\n","Epoch 40/70\n","2482/2482 [==============================] - 11s 4ms/step - loss: 0.7312 - acc: 0.6757 - val_loss: 0.8166 - val_acc: 0.6296\n","Epoch 41/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7384 - acc: 0.6765 - val_loss: 0.8180 - val_acc: 0.6312\n","Epoch 42/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7341 - acc: 0.6817 - val_loss: 0.8288 - val_acc: 0.6312\n","Epoch 43/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7253 - acc: 0.6829 - val_loss: 0.8156 - val_acc: 0.6232\n","Epoch 44/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7282 - acc: 0.6841 - val_loss: 0.8169 - val_acc: 0.6248\n","Epoch 45/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7221 - acc: 0.6918 - val_loss: 0.8109 - val_acc: 0.6280\n","Epoch 46/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7259 - acc: 0.6845 - val_loss: 0.8133 - val_acc: 0.6345\n","Epoch 47/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7247 - acc: 0.6801 - val_loss: 0.8330 - val_acc: 0.6232\n","Epoch 48/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7304 - acc: 0.6789 - val_loss: 0.8300 - val_acc: 0.6248\n","Epoch 49/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7203 - acc: 0.6833 - val_loss: 0.8284 - val_acc: 0.6312\n","Epoch 50/70\n","2482/2482 [==============================] - 11s 4ms/step - loss: 0.7179 - acc: 0.6890 - val_loss: 0.8203 - val_acc: 0.6345\n","Epoch 51/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7187 - acc: 0.6906 - val_loss: 0.8164 - val_acc: 0.6377\n","Epoch 52/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7185 - acc: 0.6853 - val_loss: 0.8225 - val_acc: 0.6329\n","Epoch 53/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7171 - acc: 0.6882 - val_loss: 0.8132 - val_acc: 0.6345\n","Epoch 54/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7190 - acc: 0.6922 - val_loss: 0.8142 - val_acc: 0.6280\n","Epoch 55/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7077 - acc: 0.6954 - val_loss: 0.8134 - val_acc: 0.6264\n","Epoch 56/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7050 - acc: 0.6954 - val_loss: 0.8175 - val_acc: 0.6248\n","Epoch 57/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7120 - acc: 0.6990 - val_loss: 0.8159 - val_acc: 0.6361\n","Epoch 58/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7033 - acc: 0.6926 - val_loss: 0.8096 - val_acc: 0.6409\n","Epoch 59/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7117 - acc: 0.6841 - val_loss: 0.8214 - val_acc: 0.6296\n","Epoch 60/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7051 - acc: 0.6910 - val_loss: 0.8150 - val_acc: 0.6377\n","Epoch 61/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7067 - acc: 0.6954 - val_loss: 0.8160 - val_acc: 0.6425\n","Epoch 62/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7126 - acc: 0.6938 - val_loss: 0.8258 - val_acc: 0.6329\n","Epoch 63/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7039 - acc: 0.7043 - val_loss: 0.8206 - val_acc: 0.6361\n","Epoch 64/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7081 - acc: 0.6934 - val_loss: 0.8298 - val_acc: 0.6264\n","Epoch 65/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7128 - acc: 0.6910 - val_loss: 0.8237 - val_acc: 0.6361\n","Epoch 66/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7056 - acc: 0.6966 - val_loss: 0.8085 - val_acc: 0.6377\n","Epoch 67/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.6987 - acc: 0.7002 - val_loss: 0.8117 - val_acc: 0.6329\n","Epoch 68/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7042 - acc: 0.7039 - val_loss: 0.8148 - val_acc: 0.6393\n","Epoch 69/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.7045 - acc: 0.6978 - val_loss: 0.8106 - val_acc: 0.6409\n","Epoch 70/70\n","2482/2482 [==============================] - 10s 4ms/step - loss: 0.6877 - acc: 0.6978 - val_loss: 0.8121 - val_acc: 0.6361\n","\n","Accuracy: 63.61%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=216, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(47, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(47, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/56\n","2482/2482 [==============================] - 25s 10ms/step - loss: 1.0086 - acc: 0.4988 - val_loss: 0.9955 - val_acc: 0.4815\n","Epoch 2/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9721 - acc: 0.5089 - val_loss: 0.9912 - val_acc: 0.4815\n","Epoch 3/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9668 - acc: 0.5081 - val_loss: 0.9901 - val_acc: 0.4815\n","Epoch 4/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9638 - acc: 0.5089 - val_loss: 0.9906 - val_acc: 0.4815\n","Epoch 5/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9655 - acc: 0.5093 - val_loss: 0.9900 - val_acc: 0.4815\n","Epoch 6/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9620 - acc: 0.5097 - val_loss: 0.9883 - val_acc: 0.4815\n","Epoch 7/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9605 - acc: 0.5129 - val_loss: 0.9880 - val_acc: 0.4815\n","Epoch 8/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9606 - acc: 0.5117 - val_loss: 0.9873 - val_acc: 0.4831\n","Epoch 9/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9600 - acc: 0.5157 - val_loss: 0.9873 - val_acc: 0.4847\n","Epoch 10/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9564 - acc: 0.5169 - val_loss: 0.9863 - val_acc: 0.4863\n","Epoch 11/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9562 - acc: 0.5157 - val_loss: 0.9849 - val_acc: 0.4895\n","Epoch 12/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9528 - acc: 0.5165 - val_loss: 0.9855 - val_acc: 0.4895\n","Epoch 13/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9527 - acc: 0.5201 - val_loss: 0.9840 - val_acc: 0.4911\n","Epoch 14/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9517 - acc: 0.5197 - val_loss: 0.9840 - val_acc: 0.4895\n","Epoch 15/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9513 - acc: 0.5278 - val_loss: 0.9825 - val_acc: 0.4863\n","Epoch 16/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9504 - acc: 0.5218 - val_loss: 0.9821 - val_acc: 0.4863\n","Epoch 17/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9518 - acc: 0.5298 - val_loss: 0.9806 - val_acc: 0.4911\n","Epoch 18/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9522 - acc: 0.5290 - val_loss: 0.9805 - val_acc: 0.4928\n","Epoch 19/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9482 - acc: 0.5395 - val_loss: 0.9804 - val_acc: 0.4928\n","Epoch 20/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9466 - acc: 0.5330 - val_loss: 0.9793 - val_acc: 0.4911\n","Epoch 21/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9425 - acc: 0.5338 - val_loss: 0.9787 - val_acc: 0.4944\n","Epoch 22/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9457 - acc: 0.5322 - val_loss: 0.9775 - val_acc: 0.4944\n","Epoch 23/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9465 - acc: 0.5367 - val_loss: 0.9770 - val_acc: 0.4928\n","Epoch 24/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9440 - acc: 0.5371 - val_loss: 0.9770 - val_acc: 0.4960\n","Epoch 25/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9451 - acc: 0.5359 - val_loss: 0.9762 - val_acc: 0.4928\n","Epoch 26/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9404 - acc: 0.5443 - val_loss: 0.9748 - val_acc: 0.4928\n","Epoch 27/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9414 - acc: 0.5346 - val_loss: 0.9746 - val_acc: 0.4911\n","Epoch 28/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9421 - acc: 0.5447 - val_loss: 0.9738 - val_acc: 0.4944\n","Epoch 29/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9413 - acc: 0.5403 - val_loss: 0.9732 - val_acc: 0.4976\n","Epoch 30/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9425 - acc: 0.5298 - val_loss: 0.9720 - val_acc: 0.4928\n","Epoch 31/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9366 - acc: 0.5379 - val_loss: 0.9722 - val_acc: 0.4944\n","Epoch 32/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9397 - acc: 0.5423 - val_loss: 0.9716 - val_acc: 0.4944\n","Epoch 33/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9367 - acc: 0.5451 - val_loss: 0.9712 - val_acc: 0.4960\n","Epoch 34/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9355 - acc: 0.5488 - val_loss: 0.9701 - val_acc: 0.4944\n","Epoch 35/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9405 - acc: 0.5467 - val_loss: 0.9694 - val_acc: 0.4992\n","Epoch 36/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9338 - acc: 0.5471 - val_loss: 0.9690 - val_acc: 0.4976\n","Epoch 37/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9331 - acc: 0.5439 - val_loss: 0.9684 - val_acc: 0.4992\n","Epoch 38/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9337 - acc: 0.5467 - val_loss: 0.9674 - val_acc: 0.5008\n","Epoch 39/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9370 - acc: 0.5483 - val_loss: 0.9668 - val_acc: 0.4992\n","Epoch 40/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9339 - acc: 0.5427 - val_loss: 0.9661 - val_acc: 0.4944\n","Epoch 41/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9314 - acc: 0.5508 - val_loss: 0.9657 - val_acc: 0.4944\n","Epoch 42/56\n","2482/2482 [==============================] - 22s 9ms/step - loss: 0.9339 - acc: 0.5463 - val_loss: 0.9650 - val_acc: 0.4944\n","Epoch 43/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9306 - acc: 0.5516 - val_loss: 0.9642 - val_acc: 0.4911\n","Epoch 44/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9292 - acc: 0.5584 - val_loss: 0.9633 - val_acc: 0.4960\n","Epoch 45/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9260 - acc: 0.5564 - val_loss: 0.9631 - val_acc: 0.4944\n","Epoch 46/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9265 - acc: 0.5512 - val_loss: 0.9626 - val_acc: 0.4976\n","Epoch 47/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9277 - acc: 0.5512 - val_loss: 0.9618 - val_acc: 0.4960\n","Epoch 48/56\n","2482/2482 [==============================] - 22s 9ms/step - loss: 0.9261 - acc: 0.5592 - val_loss: 0.9612 - val_acc: 0.4976\n","Epoch 49/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9271 - acc: 0.5479 - val_loss: 0.9604 - val_acc: 0.5008\n","Epoch 50/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9266 - acc: 0.5528 - val_loss: 0.9598 - val_acc: 0.4976\n","Epoch 51/56\n","2482/2482 [==============================] - 22s 9ms/step - loss: 0.9253 - acc: 0.5504 - val_loss: 0.9594 - val_acc: 0.4976\n","Epoch 52/56\n","2482/2482 [==============================] - 22s 9ms/step - loss: 0.9223 - acc: 0.5580 - val_loss: 0.9586 - val_acc: 0.4992\n","Epoch 53/56\n","2482/2482 [==============================] - 22s 9ms/step - loss: 0.9267 - acc: 0.5540 - val_loss: 0.9580 - val_acc: 0.4976\n","Epoch 54/56\n","2482/2482 [==============================] - 22s 9ms/step - loss: 0.9228 - acc: 0.5516 - val_loss: 0.9577 - val_acc: 0.4992\n","Epoch 55/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9219 - acc: 0.5568 - val_loss: 0.9574 - val_acc: 0.5040\n","Epoch 56/56\n","2482/2482 [==============================] - 23s 9ms/step - loss: 0.9202 - acc: 0.5604 - val_loss: 0.9562 - val_acc: 0.4992\n","\n","Accuracy: 49.92%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=72, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(234, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(234, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/80\n","2482/2482 [==============================] - 9s 4ms/step - loss: 1.0094 - acc: 0.4662 - val_loss: 1.0447 - val_acc: 0.4815\n","Epoch 2/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9770 - acc: 0.5089 - val_loss: 0.9928 - val_acc: 0.4815\n","Epoch 3/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9631 - acc: 0.5089 - val_loss: 0.9850 - val_acc: 0.4815\n","Epoch 4/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9578 - acc: 0.5137 - val_loss: 0.9813 - val_acc: 0.4863\n","Epoch 5/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9539 - acc: 0.5121 - val_loss: 0.9754 - val_acc: 0.4847\n","Epoch 6/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9473 - acc: 0.5286 - val_loss: 0.9707 - val_acc: 0.4847\n","Epoch 7/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9440 - acc: 0.5314 - val_loss: 0.9709 - val_acc: 0.4944\n","Epoch 8/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9363 - acc: 0.5403 - val_loss: 0.9594 - val_acc: 0.5008\n","Epoch 9/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9272 - acc: 0.5532 - val_loss: 0.9527 - val_acc: 0.5072\n","Epoch 10/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9206 - acc: 0.5548 - val_loss: 0.9402 - val_acc: 0.5121\n","Epoch 11/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.9029 - acc: 0.5637 - val_loss: 0.9431 - val_acc: 0.5233\n","Epoch 12/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8929 - acc: 0.5705 - val_loss: 0.9252 - val_acc: 0.5153\n","Epoch 13/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8886 - acc: 0.5729 - val_loss: 0.9231 - val_acc: 0.5443\n","Epoch 14/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8806 - acc: 0.5822 - val_loss: 0.9111 - val_acc: 0.5443\n","Epoch 15/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8856 - acc: 0.5850 - val_loss: 0.9077 - val_acc: 0.5652\n","Epoch 16/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8716 - acc: 0.5898 - val_loss: 0.9055 - val_acc: 0.5572\n","Epoch 17/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8695 - acc: 0.5858 - val_loss: 0.9002 - val_acc: 0.5684\n","Epoch 18/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8722 - acc: 0.5822 - val_loss: 0.9017 - val_acc: 0.5684\n","Epoch 19/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8657 - acc: 0.5850 - val_loss: 0.8951 - val_acc: 0.5684\n","Epoch 20/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8643 - acc: 0.5927 - val_loss: 0.8938 - val_acc: 0.5813\n","Epoch 21/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8701 - acc: 0.5870 - val_loss: 0.8972 - val_acc: 0.5700\n","Epoch 22/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8662 - acc: 0.5939 - val_loss: 0.8978 - val_acc: 0.5733\n","Epoch 23/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8648 - acc: 0.5939 - val_loss: 0.8921 - val_acc: 0.5797\n","Epoch 24/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8562 - acc: 0.5935 - val_loss: 0.8906 - val_acc: 0.5733\n","Epoch 25/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8567 - acc: 0.5842 - val_loss: 0.8929 - val_acc: 0.5733\n","Epoch 26/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8531 - acc: 0.5963 - val_loss: 0.8928 - val_acc: 0.5717\n","Epoch 27/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8519 - acc: 0.5943 - val_loss: 0.8893 - val_acc: 0.5749\n","Epoch 28/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8548 - acc: 0.5971 - val_loss: 0.8893 - val_acc: 0.5700\n","Epoch 29/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8588 - acc: 0.5915 - val_loss: 0.8906 - val_acc: 0.5717\n","Epoch 30/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8588 - acc: 0.5939 - val_loss: 0.8882 - val_acc: 0.5652\n","Epoch 31/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8560 - acc: 0.5907 - val_loss: 0.8862 - val_acc: 0.5700\n","Epoch 32/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8498 - acc: 0.5995 - val_loss: 0.8867 - val_acc: 0.5733\n","Epoch 33/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8468 - acc: 0.6056 - val_loss: 0.8867 - val_acc: 0.5717\n","Epoch 34/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8457 - acc: 0.6039 - val_loss: 0.8831 - val_acc: 0.5829\n","Epoch 35/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8482 - acc: 0.6052 - val_loss: 0.8822 - val_acc: 0.5733\n","Epoch 36/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8526 - acc: 0.5991 - val_loss: 0.8801 - val_acc: 0.5749\n","Epoch 37/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8419 - acc: 0.6084 - val_loss: 0.8840 - val_acc: 0.5829\n","Epoch 38/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8519 - acc: 0.5999 - val_loss: 0.8844 - val_acc: 0.5749\n","Epoch 39/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8500 - acc: 0.5890 - val_loss: 0.8832 - val_acc: 0.5813\n","Epoch 40/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8478 - acc: 0.5959 - val_loss: 0.8836 - val_acc: 0.5797\n","Epoch 41/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8450 - acc: 0.6056 - val_loss: 0.8837 - val_acc: 0.5717\n","Epoch 42/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8460 - acc: 0.6011 - val_loss: 0.8827 - val_acc: 0.5700\n","Epoch 43/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8467 - acc: 0.6035 - val_loss: 0.8844 - val_acc: 0.5733\n","Epoch 44/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8362 - acc: 0.6044 - val_loss: 0.8856 - val_acc: 0.5733\n","Epoch 45/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8416 - acc: 0.6064 - val_loss: 0.8822 - val_acc: 0.5781\n","Epoch 46/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8440 - acc: 0.5995 - val_loss: 0.8778 - val_acc: 0.5781\n","Epoch 47/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8435 - acc: 0.6023 - val_loss: 0.8810 - val_acc: 0.5813\n","Epoch 48/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8424 - acc: 0.6039 - val_loss: 0.8802 - val_acc: 0.5878\n","Epoch 49/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8331 - acc: 0.6056 - val_loss: 0.8790 - val_acc: 0.5765\n","Epoch 50/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8465 - acc: 0.5975 - val_loss: 0.8835 - val_acc: 0.5862\n","Epoch 51/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8374 - acc: 0.6185 - val_loss: 0.8785 - val_acc: 0.5620\n","Epoch 52/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8425 - acc: 0.6027 - val_loss: 0.8854 - val_acc: 0.5862\n","Epoch 53/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8368 - acc: 0.6217 - val_loss: 0.8809 - val_acc: 0.5733\n","Epoch 54/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8400 - acc: 0.6168 - val_loss: 0.8828 - val_acc: 0.5862\n","Epoch 55/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8391 - acc: 0.6104 - val_loss: 0.8807 - val_acc: 0.5813\n","Epoch 56/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8381 - acc: 0.6048 - val_loss: 0.8784 - val_acc: 0.5684\n","Epoch 57/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8416 - acc: 0.6039 - val_loss: 0.8811 - val_acc: 0.5845\n","Epoch 58/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8420 - acc: 0.6044 - val_loss: 0.8756 - val_acc: 0.5668\n","Epoch 59/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8303 - acc: 0.6209 - val_loss: 0.8798 - val_acc: 0.5862\n","Epoch 60/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8301 - acc: 0.6124 - val_loss: 0.8784 - val_acc: 0.5813\n","Epoch 61/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8400 - acc: 0.6031 - val_loss: 0.8793 - val_acc: 0.5797\n","Epoch 62/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8260 - acc: 0.6176 - val_loss: 0.8790 - val_acc: 0.5733\n","Epoch 63/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8328 - acc: 0.6080 - val_loss: 0.8750 - val_acc: 0.5684\n","Epoch 64/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8406 - acc: 0.6027 - val_loss: 0.8760 - val_acc: 0.5636\n","Epoch 65/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8311 - acc: 0.6080 - val_loss: 0.8826 - val_acc: 0.5733\n","Epoch 66/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8329 - acc: 0.6144 - val_loss: 0.8801 - val_acc: 0.5588\n","Epoch 67/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8350 - acc: 0.6112 - val_loss: 0.8801 - val_acc: 0.5684\n","Epoch 68/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8283 - acc: 0.6092 - val_loss: 0.8791 - val_acc: 0.5733\n","Epoch 69/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8278 - acc: 0.6164 - val_loss: 0.8797 - val_acc: 0.5845\n","Epoch 70/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8262 - acc: 0.6156 - val_loss: 0.8763 - val_acc: 0.5717\n","Epoch 71/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8329 - acc: 0.6116 - val_loss: 0.8783 - val_acc: 0.5862\n","Epoch 72/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8297 - acc: 0.6164 - val_loss: 0.8761 - val_acc: 0.5684\n","Epoch 73/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8313 - acc: 0.6140 - val_loss: 0.8786 - val_acc: 0.5845\n","Epoch 74/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8292 - acc: 0.6100 - val_loss: 0.8779 - val_acc: 0.5700\n","Epoch 75/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8329 - acc: 0.6092 - val_loss: 0.8781 - val_acc: 0.5797\n","Epoch 76/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8294 - acc: 0.6112 - val_loss: 0.8753 - val_acc: 0.5765\n","Epoch 77/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8308 - acc: 0.6172 - val_loss: 0.8769 - val_acc: 0.5813\n","Epoch 78/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8327 - acc: 0.6060 - val_loss: 0.8750 - val_acc: 0.5700\n","Epoch 79/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8258 - acc: 0.6120 - val_loss: 0.8784 - val_acc: 0.5878\n","Epoch 80/80\n","2482/2482 [==============================] - 7s 3ms/step - loss: 0.8271 - acc: 0.6132 - val_loss: 0.8749 - val_acc: 0.5717\n","\n","Accuracy: 57.17%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=203, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(231, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(231, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/64\n","2482/2482 [==============================] - 11s 4ms/step - loss: 0.9950 - acc: 0.5000 - val_loss: 0.9908 - val_acc: 0.4815\n","Epoch 2/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9661 - acc: 0.5077 - val_loss: 0.9945 - val_acc: 0.4815\n","Epoch 3/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9584 - acc: 0.5286 - val_loss: 0.9878 - val_acc: 0.4960\n","Epoch 4/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9563 - acc: 0.5326 - val_loss: 0.9805 - val_acc: 0.5008\n","Epoch 5/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9417 - acc: 0.5423 - val_loss: 0.9667 - val_acc: 0.4879\n","Epoch 6/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9263 - acc: 0.5608 - val_loss: 0.9503 - val_acc: 0.5121\n","Epoch 7/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.9035 - acc: 0.5745 - val_loss: 0.9320 - val_acc: 0.5395\n","Epoch 8/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8893 - acc: 0.5806 - val_loss: 0.9216 - val_acc: 0.5427\n","Epoch 9/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8806 - acc: 0.5858 - val_loss: 0.9165 - val_acc: 0.5556\n","Epoch 10/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8718 - acc: 0.5923 - val_loss: 0.9138 - val_acc: 0.5539\n","Epoch 11/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8641 - acc: 0.6039 - val_loss: 0.9068 - val_acc: 0.5620\n","Epoch 12/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8583 - acc: 0.5999 - val_loss: 0.9049 - val_acc: 0.5733\n","Epoch 13/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8533 - acc: 0.6072 - val_loss: 0.8982 - val_acc: 0.5797\n","Epoch 14/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8492 - acc: 0.6027 - val_loss: 0.9009 - val_acc: 0.5636\n","Epoch 15/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8485 - acc: 0.6120 - val_loss: 0.8904 - val_acc: 0.5797\n","Epoch 16/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.8420 - acc: 0.6160 - val_loss: 0.8936 - val_acc: 0.5845\n","Epoch 17/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8427 - acc: 0.6056 - val_loss: 0.8873 - val_acc: 0.5781\n","Epoch 18/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.8378 - acc: 0.6088 - val_loss: 0.8881 - val_acc: 0.5749\n","Epoch 19/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.8381 - acc: 0.6068 - val_loss: 0.8896 - val_acc: 0.5749\n","Epoch 20/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8278 - acc: 0.6189 - val_loss: 0.8876 - val_acc: 0.5813\n","Epoch 21/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8241 - acc: 0.6213 - val_loss: 0.8920 - val_acc: 0.5749\n","Epoch 22/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8209 - acc: 0.6257 - val_loss: 0.8907 - val_acc: 0.5845\n","Epoch 23/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8228 - acc: 0.6164 - val_loss: 0.8788 - val_acc: 0.5797\n","Epoch 24/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8253 - acc: 0.6185 - val_loss: 0.8773 - val_acc: 0.5797\n","Epoch 25/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8181 - acc: 0.6297 - val_loss: 0.8825 - val_acc: 0.5797\n","Epoch 26/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8236 - acc: 0.6269 - val_loss: 0.8793 - val_acc: 0.5813\n","Epoch 27/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8133 - acc: 0.6253 - val_loss: 0.8777 - val_acc: 0.5749\n","Epoch 28/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8139 - acc: 0.6189 - val_loss: 0.8711 - val_acc: 0.5862\n","Epoch 29/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.8143 - acc: 0.6293 - val_loss: 0.8793 - val_acc: 0.5845\n","Epoch 30/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8121 - acc: 0.6334 - val_loss: 0.8718 - val_acc: 0.5878\n","Epoch 31/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8088 - acc: 0.6168 - val_loss: 0.8695 - val_acc: 0.5829\n","Epoch 32/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7989 - acc: 0.6374 - val_loss: 0.8682 - val_acc: 0.5797\n","Epoch 33/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.8041 - acc: 0.6338 - val_loss: 0.8708 - val_acc: 0.5845\n","Epoch 34/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.7994 - acc: 0.6354 - val_loss: 0.8649 - val_acc: 0.5862\n","Epoch 35/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.8031 - acc: 0.6338 - val_loss: 0.8717 - val_acc: 0.5862\n","Epoch 36/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8025 - acc: 0.6305 - val_loss: 0.8641 - val_acc: 0.5845\n","Epoch 37/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.8045 - acc: 0.6374 - val_loss: 0.8613 - val_acc: 0.5813\n","Epoch 38/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7978 - acc: 0.6326 - val_loss: 0.8658 - val_acc: 0.5845\n","Epoch 39/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7951 - acc: 0.6354 - val_loss: 0.8591 - val_acc: 0.5942\n","Epoch 40/64\n","2482/2482 [==============================] - 9s 3ms/step - loss: 0.7920 - acc: 0.6434 - val_loss: 0.8641 - val_acc: 0.5829\n","Epoch 41/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7954 - acc: 0.6366 - val_loss: 0.8647 - val_acc: 0.5910\n","Epoch 42/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7922 - acc: 0.6414 - val_loss: 0.8595 - val_acc: 0.5942\n","Epoch 43/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7917 - acc: 0.6422 - val_loss: 0.8628 - val_acc: 0.5845\n","Epoch 44/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.8018 - acc: 0.6374 - val_loss: 0.8610 - val_acc: 0.5813\n","Epoch 45/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7914 - acc: 0.6422 - val_loss: 0.8619 - val_acc: 0.5990\n","Epoch 46/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7799 - acc: 0.6503 - val_loss: 0.8570 - val_acc: 0.5894\n","Epoch 47/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7847 - acc: 0.6442 - val_loss: 0.8585 - val_acc: 0.5974\n","Epoch 48/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7893 - acc: 0.6418 - val_loss: 0.8554 - val_acc: 0.5878\n","Epoch 49/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7904 - acc: 0.6362 - val_loss: 0.8584 - val_acc: 0.5910\n","Epoch 50/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7859 - acc: 0.6442 - val_loss: 0.8585 - val_acc: 0.5942\n","Epoch 51/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7758 - acc: 0.6551 - val_loss: 0.8608 - val_acc: 0.5862\n","Epoch 52/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7897 - acc: 0.6354 - val_loss: 0.8536 - val_acc: 0.5878\n","Epoch 53/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7836 - acc: 0.6426 - val_loss: 0.8529 - val_acc: 0.5942\n","Epoch 54/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7810 - acc: 0.6471 - val_loss: 0.8596 - val_acc: 0.5942\n","Epoch 55/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7863 - acc: 0.6543 - val_loss: 0.8517 - val_acc: 0.5942\n","Epoch 56/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7778 - acc: 0.6571 - val_loss: 0.8557 - val_acc: 0.5958\n","Epoch 57/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7744 - acc: 0.6523 - val_loss: 0.8541 - val_acc: 0.5974\n","Epoch 58/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7762 - acc: 0.6467 - val_loss: 0.8475 - val_acc: 0.5974\n","Epoch 59/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7830 - acc: 0.6422 - val_loss: 0.8509 - val_acc: 0.5974\n","Epoch 60/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7802 - acc: 0.6414 - val_loss: 0.8508 - val_acc: 0.5974\n","Epoch 61/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7802 - acc: 0.6547 - val_loss: 0.8585 - val_acc: 0.5958\n","Epoch 62/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7841 - acc: 0.6438 - val_loss: 0.8457 - val_acc: 0.5958\n","Epoch 63/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7757 - acc: 0.6600 - val_loss: 0.8527 - val_acc: 0.5974\n","Epoch 64/64\n","2482/2482 [==============================] - 9s 4ms/step - loss: 0.7767 - acc: 0.6362 - val_loss: 0.8482 - val_acc: 0.5990\n","\n","Accuracy: 59.90%\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=163, kernel_size=3, strides=1, padding=\"valid\")`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=3)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(162, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(162, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2482 samples, validate on 621 samples\n","Epoch 1/20\n","2482/2482 [==============================] - 49s 20ms/step - loss: 1.0414 - acc: 0.5085 - val_loss: 1.0128 - val_acc: 0.4815\n","Epoch 2/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9906 - acc: 0.5089 - val_loss: 1.0090 - val_acc: 0.4815\n","Epoch 3/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9867 - acc: 0.5089 - val_loss: 1.0068 - val_acc: 0.4815\n","Epoch 4/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9846 - acc: 0.5089 - val_loss: 1.0056 - val_acc: 0.4815\n","Epoch 5/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9866 - acc: 0.5089 - val_loss: 1.0045 - val_acc: 0.4815\n","Epoch 6/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9825 - acc: 0.5089 - val_loss: 1.0035 - val_acc: 0.4815\n","Epoch 7/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9817 - acc: 0.5089 - val_loss: 1.0022 - val_acc: 0.4815\n","Epoch 8/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9824 - acc: 0.5089 - val_loss: 1.0015 - val_acc: 0.4815\n","Epoch 9/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9798 - acc: 0.5089 - val_loss: 1.0010 - val_acc: 0.4815\n","Epoch 10/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9795 - acc: 0.5089 - val_loss: 1.0010 - val_acc: 0.4815\n","Epoch 11/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9818 - acc: 0.5089 - val_loss: 0.9999 - val_acc: 0.4815\n","Epoch 12/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9764 - acc: 0.5089 - val_loss: 0.9997 - val_acc: 0.4815\n","Epoch 13/20\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.9770 - acc: 0.5089 - val_loss: 0.9994 - val_acc: 0.4815\n","Epoch 14/20\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.9771 - acc: 0.5089 - val_loss: 0.9992 - val_acc: 0.4815\n","Epoch 15/20\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.9768 - acc: 0.5089 - val_loss: 0.9987 - val_acc: 0.4815\n","Epoch 16/20\n","2482/2482 [==============================] - 47s 19ms/step - loss: 0.9761 - acc: 0.5089 - val_loss: 0.9985 - val_acc: 0.4815\n","Epoch 17/20\n","2482/2482 [==============================] - 46s 19ms/step - loss: 0.9744 - acc: 0.5089 - val_loss: 0.9983 - val_acc: 0.4815\n","Epoch 18/20\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.9752 - acc: 0.5089 - val_loss: 0.9980 - val_acc: 0.4815\n","Epoch 19/20\n","2482/2482 [==============================] - 46s 18ms/step - loss: 0.9734 - acc: 0.5089 - val_loss: 0.9979 - val_acc: 0.4815\n","Epoch 20/20\n","2125/2482 [========================>.....] - ETA: 6s - loss: 0.9700 - acc: 0.5134Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8bHYqnWj8nQB","colab_type":"code","outputId":"579eb83a-1a3d-4e24-f825-c86ac872cc92","colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["import matplotlib.pyplot as plt\n","plt.plot(history.history['val_loss'],'r')\n","plt.plot(history.history['val_acc'],'r')\n","plt.plot(history.history['loss'],'b')\n","plt.plot(history.history['acc'],'b')\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f6a1f47cd30>]"]},"metadata":{"tags":[]},"execution_count":29},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXd4FOUTx79vEjpIB0F6VUClSRUQ\nkCIiiAgKiIgoNqzYEEVBEUWUIv4QBUQ6ijSRJlVBKaGIFEGk994DJLn5/fG95UouySW5y5XM53nu\nSXb33d25u73Z2Zl5Z4yIQFEURQkvIgItgKIoiuJ7VLkriqKEIarcFUVRwhBV7oqiKGGIKndFUZQw\nRJW7oihKGKLKXVEUJQxR5a4oihKGqHJXFEUJQ6ICdeICBQpIqVKlAnV6RVGUkGTDhg2nRKRgcuMC\nptxLlSqF6OjoQJ1eURQlJDHG7PdmnLplFEVRwhBV7oqiKGGIKndFUZQwRJW7oihKGKLKXVEUJQxR\n5a4oihKGqHJXFEUJQ0JOuf/5J/D224B2B1QURUmckFPuGzcCn34K7N0baEkURVGCl2SVuzFmnDHm\nhDFmayLbjTFmhDFmtzFmizGmuu/FdNCkCf8uW+bPsyiKooQ23lju4wG0TGL7fQDK2189AYxKu1iJ\nc+utwM03q3JXFEVJimSVu4j8BuBMEkPaApggZA2APMaYIr4S0B1jaL0vW6Z+d0VRlMTwhc/9FgAH\nnZYP2df5h3Xr0GTfOBw/Dmzf7rezKIqihDTpGlA1xvQ0xkQbY6JPnjyZuoNs3Iimaz8GACz76A/A\nZvOhhIqiKOGBL5T7YQDFnZaL2dclQES+EZGaIlKzYMFkyxF75tlnUWrLXJTOegTLph0H7r4b+Pvv\n1B1LURQlTPGFcp8L4HF71kwdAOdF5KgPjps4lSqhSeciWJG9FeJ3/Qc0aAAc9ng/URRFyZB4kwo5\nFcCfACoaYw4ZY3oYY541xjxrHzIfwB4AuwF8C+B5v0nrRJOmBueuZMHmMdHA9evAc89phFVRFMVO\nsp2YRKRTMtsFwAs+k8hLGjfm32X/FkeNDz8EXn8dmDYN6JSkuIqiKBmCkJuhalGkCHDbbfZ891de\nAWrVAl56CUhtoFZRFCWMCFnlDjDf/bffgOvxkcC4ccD581TwiqIoGZyQVu5NmwJXrgDr1gGoXBl4\n9126ZiZPVv+7oigZmpBW7o0accbqjVIEb78NVK0KPPYYUL480K8fsHNnQGVUFEUJBCGt3PPlA6pV\nAxYvtq/InJl+mnHjgFKlgI8+YjGaRx8FLl8OpKiKoijpSkgrdwB45BFg9WpgxAj7ily5gO7dgSVL\ngEOHgPfeA374gbnwBw4EVFZFUTIgP/8M/Pdfup825JX7668D7doBr77Kz9CFokWBAQOAefP44d51\nF/DHHwGRU1GUDMjVq0D79sD99zNAmI6EvHKPiAAmTQKqV2eK+6ZNHga1agWsWQPcdBMT5JcsSXc5\nFUXJgPz1FxAby9jfW2+l66lDXrkDQPbswNy59MG3bk1vTAJuuw1YuxYoUQJ4803NplEUxf+sX8+/\nHToAI0c6BQj9T1god4CTmn75Bbh4EbjnHmDzZg+D8uUD+valeT9/fnqLqChKRiM6GihcGPj+exqY\n3bsDZ5Jqj+E7wka5A8DttwOLFgExMUDdusDYsR4M9C5dmEnz4YdqvSuK4l/Wrwdq1gSyZaP/+MQJ\n4Pl0Kb8VXsodoFLftImVgJ96ijdKlyzITJmYD792rfbqUxTFf1y6BOzYwUQOgIHB/v2B6dOBqVP9\nfvqwU+4AUKgQsHAh8P77wIQJQLdubgOeeIKZNB9+GAjxFEXJCGzcSO9AzZqOdW++SauzShW/nz4s\nlTsAREYCH3xA/f3TT3TX3CBLFn7IK1cCv/8eKBEVRQlnoqP511m5R0UB335LH7KfCVvlbvH666xE\n8OKLwLVrThuefpom/sCBAZNNUZQwZv16oHhxBlQDQNgr9yxZOHv133+Bzz932pA9O9C7N036pUsD\nJp+iKGFKdLTD3x4Awl65A0DLlsBDD7HUjEsFguefZ3pShw7Arl0Bk09RFAD//BM+/RjOngV273Z1\nyaQzGUK5A8DQofz76qtOK3PmZHJ8VBSnB586FRDZFCXDI8IGDX36BFoS37BhA/+q5e5/SpRgufeZ\nMxlgvUHp0sCcOcDBgyxS4+KYVxQlXTh0CDh6lNZ7ajh7NrhSm62ZqTVqBEyEDKPcAbrYa9ZkDRoX\nBV+3LmeQrVoF9OgBxMUFTEZFyZBs3Mi/e/Z43i7CAoCxsZ63f/45cO+9nCQUDERHA+XKAXnzBkyE\nDKXcs2QBfv2VCr5jR04Yu8Ejj9ApP3kyUK8e8PffAZNTUTIcVsW/o0c9V0/csAF44AF2WvNEdDRv\nAOvWpV2WvXuBChWAiRO9G3/xYsJYgTUzNYBkKOUOAHnysHZPo0bA448z5fQG77zDi2ffPs4me/99\nddMoSnpgWe4Af3/u7NjBv56Ut4hjf18o9+nTmV7XrZubgkiE7t2BMmUcbqHjx+nmDaC/HciAyh1w\nxFFbtgR69nSqAGwMLfjt29m9acAAoE4dFqtRFMV/bNpENwbg2TVjZbNZvmxnjhxxWM5r16Zdljlz\n2K7TUhAjRyY+9uxZlqSNiQHuuw+YNcvz5KUAkCGVO8A6PjNncn6BlUlzgwIF+Eg2fTrLSw4eHBAZ\nFSVDcPIkA6oPP8xlT12LLOW+eXNCv7tltVeuTMvdZku9LMeO8QbRvj0Vddu2nAE5ZIjn8bNnU555\n8/i0//DDdO8aw+UAkmGVOwBkzQo88wywYEEiXbA6dqQl/8kn9MMpiuJ7LH97s2Zsk5mY5R4VRTfp\ntm2u2zZupDLt2RM4d44uldTyyy9087RpwyDdjz9SD7zxBvDnnwnHT5/OjLsWLegCuPdeNga67Ta6\nCAJIhlbuAK+HiAhg1KhEBgwZwkI1LgnyiqL4DMvyrlaNvmt35S5Chd2yJZfdXTMbNwIVKzJPHkib\na2buXKBkSUftl0yZgHHj2Avi009dx546RYXesSNvLjlysNfnK68Ehb7I8Mr9lls4e3XcuERaHBYr\nxibbc+bQxE+M8+f5aHbsmN9kVZSwZNMm9ljIm9ezcj96lHW7W7bkGMun7bx/9eoOazm1QdUrV5hO\n16YNlbVFjhzACy9QBzjn4c+cCcTH8+neInNm+nmfeip1MvgQr5S7MaalMWanMWa3MeZtD9tLGmOW\nGmO2GGNWGGOK+V5U/9GrF+MiiZZYfuUVpka99JIjeyY+nhfV4MFs/VSgAFO1evRIL7EVJTzYuNHh\nn7aUu3MjHcvfXrEig5TOlvvJk8xMqV6dT9h33ZV6y33JEgZG27RJuK1XL/pxnX3v06ezKmHVqqk7\nn59JVrkbYyIBfAXgPgCVAHQyxlRyGzYEwAQRuQPAAACDfC2oP2nQgOWVR45MpDmTVX1s924+gjVv\nzpzK6tXZ9PbcOfrkevRg+z4rbUtRlKS5cIG/q2rVuFymDHD1qusTsKXcy5encv/7b44BHP56a/9a\ntdiU2tqeEubOBW66CWjYMOG2QoWAJ59kosWRI0x3XLGCVruzlR9EeGO51wKwW0T2iMh1ANMAtHUb\nUwmANfd3uYftQY0xvDFv3uw5ZgKAAZMOHehTO34c6NqVs6AOHuSOH38MDBrEu/sXX6Sr/IoSsljN\njp0td8DVNbNrFw2s4sVpmcfFUYEDrv56AKhdm9krltL3FpuNv+1Wreha8cRrr/HcI0YAM2ZwH2eX\nTJDhjXK/BcBBp+VD9nXO/AXgIfv/7QDkMsbkT7t46UeXLkDu3EmntGLKFFoaf/0F/O9/3KmYkweq\nYEF2eZowQX3viuIN7pZ3Ysq9fHlmPli545bffdMmZqtY0/xr1+bflLpm1q1j6QJPLhmLsmWZ6jhq\nFPDdd0ClSunSUSm1+Cqg+jqARsaYTQAaATgMIN59kDGmpzEm2hgTfTLISnvmzEm9/OOPLBD51ls0\nzPfvdxoUFZV8etOrr9Jy+Oorf4qrKOHBxo3AzTcDRYpwuWRJPkq7K/cKFfh/sWKcnGL53Z399QDb\nZxYrlvKg6pw5/H1bGTmJ8eabNPA2bAhqqx3wTrkfBlDcabmYfd0NROSIiDwkItUA9LWvO+d+IBH5\nRkRqikjNggULpkFs/9C3L/DYY/S0DB1Kz0uVKvTCeE2FCrz7/+9/iaTfKIpyAyvTxSJLFipnS7nH\nxXESiqXcjaH1Hh3NDLXduxNOFqpVK+WW+9y59LUnV+irRg2gaVP+HwbKfT2A8saY0saYzAAeBTDX\neYAxpoAxxjpWHwDjfCtm+lCwIJ+2tmxh5tWyZWxgPi6l76Z3b+DMGWD8eH+IqSjhQUwMS31YLhkL\n53TI/fup4C3lDlC579jBKq5Awv1r1+b+3noHVq+mHO3aeTd+5Eg+mVes6N34AJGscheROAC9ACwC\nsAPADyKyzRgzwBhjOajuAbDTGLMLQGEAId+YNFMmoHFjvkaPZuaj19x9N62HoUNTuKOiZCC2buXv\nw93ydlbuVqaMs3K/6y4GMy2ry31/y+/ujWtGhD7YIkWYDeMNt97KLm5Bjlc+dxGZLyIVRKSsiAy0\nr+snInPt/88QkfL2MU+JSNiUUnzuORoPSc1fSoAx7My9ezeDq4qiJMQ908WiTBmmG8bEeFbuVlB1\nzhz62N0bUNeoweCrN66ZX36h5d6vH/sqhxEZfoZqcjz4IOM9iZYnSIx27ZhA/8wzwdeA++xZrVev\nBJ5NmzhfpFQp1/VWxsy+fVTuefJwkqBF4cJMi/Rk9QNMeqhSJXnlHh/Ptn7lyoXl5ENV7smQKRPw\n9NO03FNUOywqipZFxYq8Q1g9FYOBt99mKWMN+CopZepUtxSyVLJ7N4/VoEHCSUDO6ZBWGqT7GMt6\nT6zy4r33spTAmDGJyzBlCl1DH33EH3qYocrdC6ziYqNHp3DHvHmBRYuA/PlZ6zkt1ep8hQhn0V65\nwhl2iuItR44AnTuzqU1auHKFJXWjooAvv0y43V25O7tkLKxGGO4uHYuBA5nW+PTTwNdfJ9x+7Rpd\nMdWrc3JiGKLK3QuKFWPZmLFjU9GYqWhRtn4SYdmCQPd43L6dtbMBYOHCwMqihBbLl/PvrFnM9U4N\nIsCzz9ItOGUK89rdKViQxbq2bQMOHPCs3Nu2ZTtMT6UCAM4UnzULaN2agTPn2Ynx8Vzet4+zyiPC\nUw2G57vyA88/zwqfM2akYucKFWgtHzvG5Pm0NBNIK4sW8e+dd6YwSqyEJXv2MKjoDcuW0dqOiUnl\nDwF8/J04EfjgA5b08IQxtN4XL+ayJ+VeqRIDofnyJX6uLFmAn36iW/TFF9lbs0IFBk5ff50lgps1\nS937CAFUuXtJ06aMuzzzjGOSXP78rBfmFXfdBQwbxgv2s8/8KmuSLFzI0qg9etDvuXt34GRRAs8b\nb1D5XbqU9DgRJgY88AAVZGqywNavB15+mfVb3n036bFlyzqCXJ6Uu7dkzgz88AOfFi5fZgXHV15h\nhsSMGUFb9MsXRAVagFAhIoKTTidPpvGSKROvvSFDeL0n9nToQs+e/IH07csd6tb1u9wuXLkC/PYb\nH0OsadaLFjl6VyoZi5gY3uzj4mgFJ2ZJA7zY9+/nzaBGDSrnvXtZ18UbRGg9FypEyz05V4jldwcY\nUE0LmTKlIt0t9FHLPQU0a8ZJp2PG8FqZORMoUYLXbFycFwcwht3US5RgA+6zZ/0tsisrVzJo0KIF\nfzBly6prJjEOHGDDhdSUjvU3V68C16+n/TiLFzsypix/emIssxd9bdKErkWAxZfcSczluHw5UxP7\n9k3alWJhKfciRdh6T0kxqtzTQPbsrO67ZYvngLxHcucGpk1j5sHjj7NGRkyMY3tcHA84ZozvFe+i\nRQw0WY8ZLVvyRxeMCizQzJzJCLp7S7dgoEULBgk9ceUKg4jelLydNYs55LVqJZ85tXQpFe2tt9I4\nadyYrhnnBgjffstgqFUWwJmBA7n/E08kLxfgUO5pcclkdEQkIK8aNWpIOGCziTRtKpInj8iJEynY\ncehQEf40RCIiRG67TaRePZHs2R3ro6JE/vzTd8LeeqtIixaO5XnzeJ7Fi313jnDhmWf42Xz9daAl\nceX6dZFMmURKlvS8felSyv3000kfJzZWJF8+kcceE+nbVyQyUuTCBc9jbTaRQoVEunRxrPvuO55n\n9Wouf/+9iDF8lS7teqw//+TYzz/39l2K/POPd+8jAwIgWrzQsWq5pxFjWLv/0iU+cXrNK6+w2t2M\nGcwbLleOfsinnuLj7saNjNx64765do2WWlKP6vv3s/+js1/1nnsYcNKUyIRYvTK3bw+sHO7s2sWS\n0vv3M33LHavO+c8/J52V9dtvLG7Xrh2t8Ph44PffPY/dvp0pvFYDaoB56tmy0XqfPh3o3p1ZB4sW\nMcXw9dcdYwcOpCumZ0/v32epUnwKSO+4VDjhzR3AH69wsdwtXnuNRsv69T486Jo1tN7btaP15MzJ\nkyLjx4s89JBIzpy0curXFzl+3POxRo/mmO3bXdc3ayZSqZL3MsXHi/z8s8jlyyl7L6FG4cL8vO69\n1zfHO3064XeYGqZNczzZLVyYcPvDDzu2J/XU16uXSNasIpcu8bvMnFnk9dc9jx0xgsfbu9d1fZcu\nfNKMjBRp0IDHEhF54w2Onz9fZPNm/j9gQMrf6/XrvvnMwgx4abmrcvcR58+LFCkiUqZM4vo1VQwZ\nwq/pyy+5vHu3SM+e/DECIkWL0oUweLBItmx8XN+yJeFx2rUTKV484Y/l8895nP37vZNn2DCOf/75\nNL2toObMGb5HY/j5ppW9e6lIx4xJ+7EsFwog8tFHCbeXLMkbdmSkyNtvez6GzSZSrJhI27aOdQ0a\niNSs6Xl827a8sN1ZvJhy1K7t6oaJiRGpUoU/iPvuE8mVi5+p4hNUuQeAtWupX2vX9qFhGx8v0qoV\nlXn79vTPZ85Mhb5hg6uyXr+eyihnTpE5cxzbrl8Xuekmz/7L7dt5GYwenbwsW7eKZMnC40dEiPz1\nl2/eY7Bh+Yjvvpt/z55N2/HeeYfHadIk7bK1acMnrQoVRB580HXbiRM8z2efiTRuzDiOJ9at47jx\n4x3r+vXjd3runOvYuDiR3LlFnnoq4XFsNsZt3PcREdm0ibEBIPGbjJIqVLkHiFmzaPC1a8ffhU84\neZKWVs6cfOQ9ciTxsYcO0QIDqNCrVRNp3pzLM2YkHG+ziZQowWDuqVOJH/faNZGqVUUKFuQNIV8+\nKpBwfGy2goVffMG/f/yR+mNdv04XT0QErenTp9MmW+nSIo88ItK5M68JZ+bPp7wrVjiesHbtSniM\nPn0SyrJ8OcfPnes6dv16rp8yJeWyfvEFnxaPHUv5vkqieKvcNaDqYx58kD06Zs1iTOnECfYMmD6d\nk6COHk3FQQsUYFPuQ4eAwYMd/SY9ccstzGf/6iumWhYuzMkm5cqxUp47xrBZwdq1zH3/8ksG7Nzp\n14+d6seO5QzXAQOYRjlrVireUJDzzz+c+HL//VxOS1B1zhz2aezXj0HLefNSf6yLF/ld3n47JxId\nOuTaiH39en6f1auz9op1fndmzeJUfOd88zp1OF3fPSXSOb89pbz6KoOr7vXWlfTBmzuAP17harlb\nvPyy3IhrOb+yZRPp3TuFaZPpwd9/M3gI8LH/yy9Ffv2VvvgVK/g44uzWiY0VqVxZpFQp+ljDibZt\n+RnExfELe+211B+raVP6wWNjRW65hY90qcVyF82ZI7JyJf+fN8+x/YEHXF0xd9xB15IzO3aISwzH\nmXvu4ZOeRXw8ffGVK6deZsXnQN0ygSUujinSI0bwSffvv+myfvxxPqHnyCHy3nv8/QQNNhsVR7ly\nCe9K5cqJXLzoOn7JEm4bONCxLi6ObqPffhMZN47+5mHDguyNJkPFisxCEqGya9kydcfZuVNcAp8v\nvMCbhXtAxmZL2tVm8e23PN6ePQxgGiPywQeOY9x8My8wi/fe48VmWRJXrtCVFhkpcvBgwuP3789j\nnj7NG3bHjjzf4MEpf++K31DlHsTs2CHSoQM//bFjAy2NB2w2kcOH6Yf9+muRN99MPHj64IMMspYt\nK5I3L5WD803Byux49FH67YOd69eZfvrOO1zu3DnxCUPJ0bs3j2Up7l9/5Wcxe7bruIEDuf6bb5I+\n3ksv0SqwbpS33SbSujX/P3iQxxgxwjF+wwauGzeOyrpFC34/kyZ5Pv5vvznGN2ggN4Kz4RhXCWFU\nuQc5NptI3bqMtSU2MTAk2LePCr5TJ1qm/fqJjBwpsmAB0zavXxf59FNeas2bJ7T+gw3LbTFhApc/\n+ojLKZU7JoZB54cfdqy7fp1Tmbt1c6zbvZs3xxw5qHjHjUv8mI0bMxXLomtXphuKMJLvnttuszGg\n2aoVXTbJWRNXr/LJwhhmZE2blqK3rKQPqtxDgLVr+Q306RNoSdKBceNoxd91V/oFHMaOZcZGbKz3\n+1hKct06Ls+cyeWUzk6bNIn7LVniuv6xx6j0Y2OpfFu2ZB74f/8xP90Yx43FGZtNpEAB15TE4cN5\njsOH+aQRFUXXizMvvOB4iho1Knm5W7fmDWjlypS9XyXdUOUeInTtSsNtz55AS5IOzJ3LyTzVq/vf\nRfPbbw4XUb16VJ7eMGgQ9zl/nstWjZPvv09+34sXRVatomukYkXGKdxjDTNm8HjLljn+HzqU2y5f\npnUeESEyebLrfkePcuzw4Y51q1fLjQBrs2ZMVXVn9Wrmmw8b5t37P3cu7email9R5R4iHDrEGdzO\nT+9hzezZvOzefNN/5zh3jn7ysmU5KzR3bs4R+O675P3H3bq5zkqNjaVyfOutpPd7+mnXeEOhQiI/\n/ZRw3KVLvMF1787smTvvdH2yuHRJpGFDukUOH3ast/z1y5Y51l2+zBvBu+8y3pFYkS13a14JaVS5\nhxADBsiNuScZgp49qQiXL0/ZfmfPUlknZ/V36UIX0Jo1XN6/X6RRI37IjRoxHpCYkq9dO+FM0ipV\n6LNODMu/1qkTn04OHUr6JmL5vxObILV7N5V2796OddaEKneX1u238ynBm4CsEhaocg8hLl9m3Kt0\naZH332c9qLTOeA9qLl3i9PlixVxrjuzbRwvZk387Ls4x09ZZ6bkzZQrH9O+fcP8vv6S1DDAHfNIk\n12nENhutfPe6OR078ikgMVq1oh/d28j4uHGSbDnbTp0YZLVcJN27M/ruzhNPOG4UGzd6d34lpFHl\nHmIsXUojzPnJvn59kUWLwjQTbf16BgAfeYTuhxdecNQiyZ2baXzOWPVZatTg3wULEh5z/37uW7du\n4kHUa9dYU6VSJR7nvfcc2yy/tnM6oQhzyY3x7N6w6rQ45/onx+XLfFxL6g7+11+uN6maNT1XqBw5\nkuOyZmU2jhL2qHIPUc6fp3u1f3+WfLHqV6XUgxESWPndmTJR0T/zDN0UJUvSEv77b46zMlaefpoK\ntkoV+rSda5b89Ret65w56dZIjvh4WuTZsjl821Z9FffmJT/8wPWbNiU8TuvW9HdbAVhf0rq1SP78\nfCLIlk3k1VcTjlmzhrLVqeP78ytBiSr3MODqVZGvvmJ8D2D11LDKqomLo3+8e3fXbJbdu/mmCxVi\nADZXLpFatfiBiHCqb9asnJQTHy8ycSKVX9Gijs5A3vDff7yxWO6RUaP4QR844Dpu61aud89giY7m\n+g8/TPl79wYrG8ZKZ/SUAx8Tw3Srl1/2jwxK0OFT5Q6gJYCdAHYDeNvD9hIAlgPYBGALgFbJHVOV\nu/fExLDses6c1GGffJIBnsB37KByB1iJ0l3hWoq4Xj3+bdiQbpWU8tJLDF7u2EEF6TwD1OLaNQZo\n+/Z1Xd+mDXPCPZW89RUNGzp8dYnl2q9fr+mLGQifKXcAkQD+A1AGQGYAfwGo5DbmGwDP2f+vBGBf\ncsdV5Z5yDh7kZFCA/vlt2wItkZ/ZsoXKzdOEGpuNRbgAFvZK7d3uxAk+GTz4IJ8Eqlf3PO7WW12L\nfllT+90Dt75mwQKex5jw734Vohw/zukQiXHhgm8TJLxV7t6U/K0FYLeI7BGR6wCmAWjrXlwSwE32\n/3MDOOJNRUolZRQrxmqts2axdHCXLqwiG7bcfjvLFzdsmHCbMcCUKSyF/PnnLNGbGgoWZMnj2bPZ\nV/TWWz2Pq1SJtZvfeAOoXZuvPHmAl15K3Xm9pUULoFo1oGJFIHt2/55LSTEXLgD16/Mr2rAh4faN\nG4GiRVld+fbbgWeeYdvZw4f9L5s3yv0WAAedlg/Z1znzAYDHjDGHAMwH8KKnAxljehpjoo0x0SdP\nnkyFuArAmvEjR7K8+pgxgZYmgGTNCtxxR9qP88orrJEfE5O4cq9Wjb/IESPYVPzNN9lQOk+etJ8/\nKYwB5s4Nz7r5QcrBg7x39+jBcviXL3seJ8Ke33v38jJ44AGW2LfYv58tAfLlAz74gMbZ9OlAt278\nSv1OcqY9gIcBjHFa7gpgpNuY1wD0tv9fF8B2ABFJHVfdMmnDZuN8nPz51d3qE775hu4PT7NKRZil\ns25d+NWuVxLQuzdDLLlzy40s0wcfZFjGGSvsM2gQPYi5crECxMWLnL5x2208xtatjn3i45kElpby\nSvChz70ugEVOy30A9HEbsw1AcaflPQAKJXVcVe5p56+/GAsM517V6UZ8PGu0hH2kWkmKixepkDt2\nZBx9yRLG3PPmZUWIgQN5iWzcyCSlli0d8ff58/l7bNOGfU8yZXKtFuErfKnco+zKujQcAdXKbmMW\nAHjC/v9toM/dJHVcVe6+oVcvXlCbNwdaEkUJfSxr3D2j9uhRRw+GO+/klIpbbklogY8YITcmISZW\nNj+teKvcDccmjTGmFYBhYObMOBEZaIwZYD/JXGNMJQDfAsgJBlffFJHFSR2zZs2aEh0dney5laQ5\ncwaoUIHxvpUr6aJVlIyGzQacOgUUKpT6Y4gAlSszbm21o3Vn1izg+eeBkyfZQrhBg4THGDKEfvYe\nPVIvS1IYYzaISM1kB3pzB/DHSy133zF6NC2FJ5/URvNKeHL5Mq1p51JAzjz1FLNFH3nE1cedEqzC\nm8lVdz53LrBpyPBhKqQS5PRdSGzRAAAgAElEQVToAbz2GlOsypcHPvkEuHo10FIpSsq4epWZKtZr\n925mg7VpA+TPz5TDJ5+kle7M2LEc16QJ8MsvTDns2BHYsSPpc7kfZ8QIWv6PPJK0nLlz80k56PHm\nDuCPl1ruvueffxjMAURKlWKRRUVJC/Pni/z+u//Ps2UL+3u792UHWGropZc4gRgQefFFRzE9K7B5\n77206k+d4kTiXLk4edhT3/FTp1iQ9LbbWIFVhBUvjHGtIxesQGvLZFyWLGEDkA4dAi2JEsqcPs00\nwIgIkcGD/VeddN06ZqMULcp+7GPG8DV2LDPCrPPabExTtIp5njnDMtmeApv//EOl76kJTteurFNX\npgyP1aoV+7dHRbn2RwlWVLlncPr357f722+BlkQJVaz+IE2b8u+jj7IUf2JcuMD2r1Z9N29YuZJW\ndunS3nVCtNlEevSgPOXKUSF76nci4ig6OmuWY90vvzhuDlevigwZInLTTVzXubP3cgcSb5W7V9ky\n/kCzZfzLlSucsV6oECP/ERpdUVKACCfr5s8PrF4NDB4M9OlDf/bs2UDp0q7jr10DWrUCli1jWYyJ\nExNmm6xcCcyYwYnFViWFzz4DSpQAlizhDE5viI8HOnUCfvwRGD488QoQsbFAjRrA6dPA9u1cV6UK\nfeYbNgBZsnDdiRPAt98Cjz0GlCzpnQyBRLNlFJk8mRbJd98FWhIl1FiyhNfOhAmOdQsW0I9doICr\nH94qjQ84Yj7uBTSnTKGVnS0bX5Y/vWZNFt5KKdevsxhmcq6itWvpVnr2WXZ3jIjgulAG6pZRbDa2\nBC1ShDPvFMVb2rdnaQv3ags7d4qUL8/ZmuPH8xrr1YuaxPLLP/20uLR0HT6cy40aOaoj22xMb0yP\nLmOvvuq4mbzxhv/P529UuSsiIvLnnw5LKizb9Sk+5/Bh1lZ5/XXP20+fZg9xgNPsrarL1vUVG8tp\n+ZGRbAULsDZLoMryXLpEn36FCp47JYYaqtyVG3Tu7LBcMmdmAKtsWQampk5N3WOxEpps3Sry7rtJ\nP8kNGMBr5d9/Ex9z/Tq7IgJspuXe3+TCBZFq1bj9qacSb2mbXpw549+eKumJt8pdA6oZgPPnOdHj\n/HkGvq5dA/btA1asAM6d45hChYCoKL4iIxkwa9SIr9q1GQRTggOR1JWZ2L4duOceTp2vVYsTfgoU\ncB0TF8fvvlIlYNGi5OXYsoVT9qOiEm4/dQpYtQpo21bLYvgSbwOqqtwzMPHxbCawdCmwZw+X4+P5\nA9+6lT9cEWYV9OwJDBwI5MoVaKkzNrGxQPPmQN68wNSpjoyP5Ni1izdqAOjblz1HSpYEFi9mtorF\nnDnsFzBzJtCune/lV9KOt8rdw/1WyShERgJ33cWXJ86eZT+KuXPZHGTOHGD0aKBly/SVU3EwaBCf\nuACgQwemFmbO7Dpm61YgRw4q7chI3ribNOGNe8UKWuV33MFp/fXr87vdupXpiH/8wZTEBx5I73em\n+BxvfDf+eKnPPbRYvZptRAGRxx4TOX8+0BJlPDZtYjphp04iX33F76J9e4c/e9s2BjKt+EqWLCJV\nqnBaf758nO3pzObNrlP+q1ZlNsn27en/3hTvgZc+d7XcFa+oV49t/QYOBD7+GDh+nD7b1LYuVVLG\n9etsz1agAPDll5xcdO0aC8Z17cr1o0bRbTZ4MN02O3cC//zD/4cOTdiR8M47OZknOhqoW5ftZJXw\nQZW74jVZsgADBgClSrES5Qsv0E2jwTLf8uWXwPjxVObdunFG5UcfMQYyZw4VOwC8+ioVfJ8+nIH8\n7LNA//4Jg6RJUbQo3TNK+KEBVSVV9O1LC37wYAbnFN/wyy/0dxcqxKejHDmAhx4CpkzhtP7vv0+4\nz5w5QLlyzFpRwh9vA6pacURJFR9+yLrXb77JoJ7iPefPA59+ynRUZ3bsYM2UqlUZBI2OZl3yH38E\nbr4ZGDbM8/HatlXFriRELXcl1cTEAE2bAps20WVQvnygJQp+Ll4EWrQA/vwTyJYN6NePfvNLlzif\n4MIFKvXixR37nD3LxhKWO0bJ2KjlrvidbNmAn37i/598ElhZQoHLl4HWrYF164Cvv2ZKaZ8+DGy2\naQPs388enc6KHWBAVBW7klJUuStpokgR4Kmn2OLv4MFAS+M7Dh8GXnyRgWPr9d57dKm4c+EC8PLL\nVNSXL3s+XkwM3SerVgGTJwPPPMOJQvPmMSi6ejUVfr16/n1fSgbCm3xJf7w0zz182LeP+dcvv5xw\nW2ysyNmz6S9TWoiLE2nQgHV4ihVzvCIi2C1o7lzH2BUr2NLQGOaKly7NcrkW8fEiq1ax4YUxriV0\nLa5cYbs4RfEGaINsJb0oWRLo3JkND06edKy/dg24916gbFm6HIKNy5c5fcedoUM5M/ebb1wbNq9Z\nQ/dImzYMfL72GtC4MWeBrlrFZhRRUXzPPXowi6hUKeDuu2mZjxnDnHR3smUDqlXz+9tVMhre3AH8\n8VLLPbzYvp2W6bvvcjk+XuSRR2jNZssmUr9+4CsDWthsIqNHcwZnu3asYGixZQst9gcf9Fwi+do1\nkQ8/5BhA5LnnXCssXrki8tZbLHebKZNI69Yikya5nkNR0gK05K+S3rRrx049589zGjsg8sknVG6A\nSL9+gZaQirhLF8pTowaV8G23sQnFtWsid94pUqhQwobL7uzcmXjvThHWRD9zxreyK4qIKnclAKxb\nxyuqfn2HVWtZv48/Tp/1ypWBk2/bNipyY2h9x8eLLF3KjkO5c/PmBLj61BUl2PBWuavPXfEZd91F\nf/Pq1ZxlOWKEozTByJFAmTKcZXnmTPrKdfUqp+VXr85myb/+Crz7LqfsN2nCvPLSpZmG2KOHVkRU\nwgNV7opPGT6cNU+mTnVt4JArF9cdP86gYmys784ZFwf88AMnAd18M/DEE0wzvHSJ9cpvvx344APW\nKd+8mROvnClVijek77+n/IoSDng1Q9UY0xLAcACRAMaIyCdu24cCaGxfzA6gkIjkSeqYOkM1YzJq\nFPD888yumTCBmSap5fhxTs3/4gtg717WV6lRgwr97FlWrIyN5czZr74CmjXz3ftQlEDhs2YdxphI\nAF8BaAbgEID1xpi5IrLdGiMirzqNfxGAJnYpHnnuObb2e+cdFsVKSVXJq1eBSZOYpvjHH8Du3Vxf\npw4wZAgnCUVG0pJfvZoThAoV4mQkbROoZDS8KflbC8BuEdkDAMaYaQDaAtieyPhOAN73jXhKONKn\nD2usDBpEd82QIVTwixczN7xZM5a4dVbIBw+y7duGDaw7Xr8+W/81bkxr3fkGERXl6P+qKBkVb5T7\nLQCcJ5YfAlDb00BjTEkApQEsS7toSjgzcCB94l98QaV++jRw9CiQPTuLkM2fD0ycSMW9ciVbyl27\nxqCnNlxWlOTxdbOORwHMEJF4TxuNMT0B9ASAEs5deZUMhzFU7EePskdrbCybTMTGctvhw3S3dOjA\nYGn58sDs2UDFioGWXFFCA2+yZQ4DcK5TV8y+zhOPApia2IFE5BsRqSkiNQtqT68MS3w8MH0665bP\nmAFUqQJs3MjSBQcOMF3ywgUgXz5m2Nx/P7B2rSp2RUkJ3ij39QDKG2NKG2Mygwp8rvsgY8ytAPIC\n+NO3IirhxMKFTE189FHWKJ8yhSVwq1bl9ptuojtmwgTgyhUuN24MZM6c8Fh79zKwqihKQpJV7iIS\nB6AXgEUAdgD4QUS2GWMGGGOcuy8+CmCaeJNbqWRIfvyR9cxFgGnTgL//ZgEuT+mQXbuyCUiNGsyb\nL1+ehbxiYuieadmSBcnuvhs4ciT934uiBDvaiUnxKf/9x+bOTz7JWZ8W06fT3VK3LoOluXJ5dzwR\nYNkyzihds8aRu37LLZxJ+vXXwHffceKSomQEtBOTEhCee45pjBUqcCr/nj30m3fuzPTFBQu8V+wA\ng6tNm9L98ssvvGnMns3+o199BRQuDCxa5Le3oyghi6+zZZQMzJIlrNvyzjvMY//mG07pFwEaNuSk\nohw5UndsY4BWrfhypnlzPgnYbKwVoygK0Z+D4hNsNuDtt4ESJdiObsQIWu0vvkirPS2KPSlatGCO\n/MaNvj+2ooQyarkrPmHGDM4eHT/eMbO0aFF2NfInVr2YRYuAmsl6IRUl46CWu5JmYmOBvn2Zr/7Y\nY+l77kKF2KLOk9/95ZeBbt04s1VRMhqq3JU0M3Ysi3h9/HHaqjymlhYtgD//5MQniw0b6BqaMIE9\nT69cSf448R7nVSuKGz//DLRvH/RWgyp35QZHj7Lq4tNP868ndu1icLRhQ6Y2vv02G2HUr88c9kDQ\nogUrQS5zqmj03nuc4Tp8OIO899/PIG9ibNjALJ5ff/W/vClm4UL6u5TAEx8PvP46GwYMGxZoaZLG\nm3ZN/nhpm73AcOmSSJ8+IlWritSpI9KkCZs4V6nCFnMAG0cDIkOHuu67e7fILbewLV2DBiKlSolE\nRbEP6erVgXk/Iux9mjOnyLPPcnnVKsr/6adcnjKFMtatK3LunOdjNG/OferW9dwYO2CcOsUegMaI\nrF+f/udfvpzdzxUyYwYvlBIlRHLkEDl4MN1FgPZQVdyZPZvXJECl3ry5yN13i1SvLtKsGZXhhg0i\nV6+KPPQQx33xBffdt4/75ssn8tdfjmPGx7PpdKB54AGRMmX4f+PGIoUL80Zm8dNPIpkyibRokVB5\nr1zJ91qtGv+uWOFnYfftE6lQQaRoUcerRg2RAwcSjn3tNTafzZdPpHZtfuDOnDvHL++rr5I+58mT\nIuPH84utUYN36uTYtUskc2Z+mMeOef/+EmPQIF543pw7GLHZRGrWFClXTuTff0WyZhXp2DHdxVDl\nrsiFCzT2Jk2idQ7QQv/tt+T3vX5dpH177tO3LxVnnjxU/sHIl19S1tGj+XfEiIRjRo7ktlGjHOts\nNj6FFLk5Xk53ekEKFYiT5s1TcOJLl0RmzRJ56imRb7/1bp8hQyhI9+7cr0cPPno0aCASG+sYt2cP\nleuTT4pMmMB9xo51bI+LE7nvPq43RuSXXxKea9s2kUaNeIMAeCPJk0ekdGmRQ4cSl9FmE2nZUiRX\nLiqxFi0S3lhSwpQpPH9EBJ9EZs1K/bECxZIlfA/ffMPlDz7g8pIl6SqGKvcMhM0mMnEijbyHH6aB\nd/PNcsPNAlB3DBlCpe0t16+LdOjA/XPlElmzxn/vIa3s2kU5M2USKV6cTx/u2Gx8WsmeneNFRBYt\n4n4j75snAsigWjMFEImOTuaEa9aI3H+/w4cVGUkf1aZNyQvbtKlI5cqu6yzl/cEHjnWdO4tky8ZH\nf5uNj1kFCoicOcPtb7zheLyqVk3kpptcXSh//kmLv1AhkX79+KZsNt7xc+USqVSJFr0nZs1yHHvU\nKP7/2WfJvzdPREfzBtGggcg//9D6BUR6907ZBRlo7r1XpEgRx8V15Qpvkrfdlq7vQ5V7BuJ//+M3\nmS0bn/abNqVR+PHHdEds2+ZZ2XlDbCyfptet863MvsZm4+8MEPlmVCwfV2rX5iPLkSM3xh06JJI3\nL+MNsbEid90lUrJEvFzNe7NIZKScy1RAct8UL+3bJ3GymBjeQQoXFnn5ZZFly+i2KFyYSjapH/rF\ni7wDvf56wm1du9KyXbmSChEQeecdx/bNm7n9hRccN4Pnn+e2/fupxMuVo/JfsIB3sbJlRf77L+G5\nVqygwq1ZU+T8eddtV66IlCzJG9D16/xwH3qINy/nC+H4cX7Offpwe+XK/AxeeYWuJxGRo0cZqClZ\nUuTECa67epVyAyKtWnl+Ijh6lIGhxG4o06aJ3HqryJYtnrcnx4YNIvPn87OJi+O6EydEvvtOpF07\nfr/vvcfPQoQ3REBk8GDX48ydy/VDhiR+rj/+EFm4UGTv3rQ9/dhR5Z5B2LWLv+FmzXxy3YQ077x+\nTW4vckKuF7Nr+YoV+eEULkwFbGfqVG62gqhjH7Gb75MmiURESN+6S8QYhxFss1EXzJ1r1wOffsrx\nTscUEZGZM7l+4MDEhZw9m2OWLk247cIFKudixUTq16eV7h4B7tWLCj5LFgYXnG8kq1fzxnHHHVTE\nVasm7SufN4/jatYU+fVXRzCiXz9JEHw4fZoKr0wZkU8+oXzGcFxUFK2KBx5w3AQiI/nkUbs2v4PN\nmxOef/hwcYl8W8TH84K2Hjvd4wmW3ACt5suXE3+Pnpgxw+GmsjIIypZ1vJ9ixWilA3y/8+fzkTh3\n7oQ3QhHeoG66yfFE5czevQ5ZAd5Qb79d5McfUyazE6rcMwCxsbRA8+RJ2n2aITh8WKRKFbEB9DH/\n/DOVxNattPAiIkQ++ujGHfCRR3j1lytrk9iiJUTuuUesDSdylJJs2Wzy8MMin3/O36L127yzSqys\nyNGKLhlPdOxIP/m2bZ63P/MMfWTXrnneHh1NBZ1Y4ODsWVropUszk8adsWO57z33JJ4a5MxPPzl8\neFWrMniRJYtIp04Jx/7+u0MpVq9OF1J0dMInlQMH6CPMmZNjZ8zwfG6bjYGdqCiRtWsd6wcP5n4j\nR4q0acP/J07ktuXLqSBr1HBkrjz9dPLv02LhQn6+9erx5vXtt3yKeughvp+NGx03uaVLee1YX77z\nU5Qzmzdz+3vvJdzWowevh59/pq++d28+TS5Y4L3MbqhyzwB89BG/wSlTAi1JgNm5k4/9OXN6/tFc\nvEgrEuCPOCZGTp+mcbjwtYVcv3Ahx27cKALIy/XW3fhN16pF19fkySIlcp0WQKR9s3Oyd68HWY4f\nZ65o7dqOx30Lm40pR23bJv1+xo2jFZzYDeDgQVrSibF5c8r8cFeviowZ41BkOXPyZumJv/7ynNXj\nibNnk49BnDnDz6RMGVrFa9dS2bdvz88rJoZPKJGR9DPmzOkaK3jrLcr8ww/Jy7NqFX2Xd95J2bzh\n2jX6JevV43ebGO3bM47h/L389x/l7tXLu3N5iSr3MGfjRv4GHnkk0JIEmOhokYIF6cJIKg/cZmPi\nvmXZnztHK75iRVqszvmRLVrImQLl5fNPrrsa4Hv2yJWoXDKg+izJnp1P7zExHs5lZYZYeaQW27Zx\n/ddfp+EN+5H4eLogvEmn8iWrV1MJtm9PJV+ihKuL48IF3mGBhFk+169zW+7cdIEcPUpr/IEH+HTx\nyCN0M339NceUL++btE53tmyRG6llFt278ykosRtlKlHlHsZcukTjpUiRpA24kMAK2HnixAkGDxNL\nXZk/n5ZcyZK03r1h8mSHP9rKApk2zXXM8uVc/7//ua7v1ImW36FDsnQphwwf7uEcNhv9sDlz3gjm\nLl4scnGg3ce8f793smYkrMfQyEha2O6cPs3sIE/B4f/+o887b1658bhVqhTTN8uUcfjSixf372ff\noQO/85MnmQcfGcmAu49R5R6m2Gwijz3G63Xx4kBLk0auXKEroHHjhP7hc+doeQH0Wf7vf46bQFwc\nLSSAj9gpDThYmSQAA2nOueUiPE/t2lQG/frxZaUdvvvujWGNGtFdbSVUuPDvv5S7SxeZPt1u1JWa\nxLuykpC4OPrOR49O3f6zZ/ML+egjWtHOBkNMDGMvFy74RNRE2bqVP8y33xbp1o2xAadMLV+hyj3E\nWbOGbkR3o9YyNgcMCIxcKebcucSnr1tZGVFRVNJHj3L95cvMiY6KYgaLNVGnUyfObmzcmMs9eiSi\nWb1gzRpadVOnet6+cCGVs/NkgcqVXbIlVqwQj96XG7z7rhxDIcmf+zpnrGOfxL/mIQVSCR8efZSG\nQ2Qkg8p+QJV7CLNtG2MzAJMyrJv/unXUN/fdFyJpjzabSMOGzE5wf9TevduRlbFoEet0lCnDG8F9\n99ECstwl8fG0yKxMjWzZmI8cBDRpwkxLT9l4tkuXpV22BZLZXJN3H97B7MkhQTrFV/EN27fz2s2W\nzT++fVHlHrKcPk1PQeHC1GdZs3KS4dixjDOVLOk5Ay4omTxZbkxvLVTI1d/5wAOuWRlr1zLLJDJS\nXKZ4O7N0KYNuqZ244gd+/10SncNixVU/xRty+eYykgvn5YmucQkHKuHFoEGpdy95gSr3ECQ2lrNL\nM2fmpDYRkR07OIvScj0HojCgR2w2puQtWcLJO+65zufPM+JbsyZ9kTfdxCDmpUuchOJpOvuOHfSz\nDxuWfu/DB9x7LxN2nAuVHTlizYS1SVyzliKA9CixWHLmdB2nKClFlXsI8tJL/EbGjXNdHxtLfTdn\nTjoJcuKEY6q4M1evUrg6dehGcfZHt27t6pvo3Zvrrckpv/zCx9X27floks71OPzJ6tV8qy1aMAzw\n+OMMIWTNylIqsnOnSM6csrLvIgFYOUBRUou3yt1wbPpTs2ZNiY6ODsi5g4UDB9jYedMmYP16YMEC\n4JVX/N93NEni4oDy5YH9+4G6ddnGqFkzdrEYPpwdPW6/HWjcGLj1VqBiRWDrVgperx671Bw9Ctx5\nJ3vcjRnjOPZnnwFvvsn/lywBmjYNzHv0A926sadGVBSQKROQOTPw7rvA44/bB1y9ClumLChX3qBs\n2SBtCqKEBMaYDSKSfMdgb+4A/nhlBMv9wAHPxumFC7TuLKM3IoKG7CuvJMzKS3es2iePPcYp3s7W\n+b33MvjpKS/9xx/pN6pShdUL8+RJaP3bbEwp9FQ0K4Pw/vt8gPF2kqeiuAN1ywSGq1eZvVenjtyY\nUDdmjEPJr1/P2lARESJvvsmqrEHlg23RglX8rLvMwYMi33/vXSnbJUsc9USSax6RQdm9mx/Pxx8H\nWhIlVFHlHgCGDmVgDWCRvP79HaWrS5XiZMtMmThtfeXKdBRs2zY+FiRXPc/SPM41xVPKxo1M83Gv\nq6Lc4O67OXfL23Z+168nXmZGyXiock9nrHLPjRtz5qiVh26zMZZolcZ46KF0LhkQF+e4wyRXPe+N\nN5iKmOFLTPqXb7/l15EnDxOKypRhRtT777tOrtyxg/Ng8ufnk6CiiHiv3L0KqBpjWgIYDiASwBgR\n+cTDmI4APgAgAP4Skc5JHTPcAqr33QesWwfs3QvcdFPC7SLAnj1AmTKAMeko2LffAj17Mtj5xx/A\nDz8AHTokHHf1KlCsGNCoEfDTT+koYMbj6lXg00+B06eBmBi+9u8HVq/mdVK+PFCoEJejohi33roV\n2LkTqFAh0NIrgcZnAVVQof8HoAyAzAD+AlDJbUx5AJsA5LUvF0ruuOFkua9aRUvMvedAwDl9mmZf\nw4Z8rq9d21E9z52JE/kmfv013cVUyLFjLF7YrBnj0oMGcd2BA+qnVxzAV5a7MaYugA9EpIV9uY/9\npjDIacxgALtEZIznoyQkFC1366Nyt7ybNAG2bwf++w/IkSP95UqU556j5b5pE9MX9+4FqlYFKlUC\nfvuNOXsW9esDJ08C//wDREQETmbFI3XqALGxwIYNgZZECTTeWu7e/IpvAXDQafmQfZ0zFQBUMMas\nNsassbtxPAnV0xgTbYyJPnnypBenDi5efpmPyM73pGXLgOXLgXfeCTLFvnEjMHo00KsXFTsAlC4N\nfPMNsGYN8MILfO4/dQrYsoUum2efVcUepLRvz690795AS6KECt5Y7g8DaCkiT9mXuwKoLSK9nMbM\nAxALoCOAYgB+A3C7iJxL7LihZrlv3QrccQcQGUn9N2wYdeHdd3My0r//AlmzBkCwa9eASZOA8eOB\nXLk4qahiReC774B9++iozZPHdZ/nnwdGjXIsZ87MN3X4MJAvX3pKr3jJnj1A2bLAkCFA796BlkYJ\nJN5a7lFeHOswgOJOy8Xs65w5BGCtiMQC2GuM2QX64dd7KW/Q8847DJSuW8fJmM8/D0ydSoP366/T\nQbHHxgIXLzqWY2KACROAESOAY8eAypWBS5eAlSuBK1c45vvvEyp2APjqK2qInTsdrxo1VLEHMWXK\nANWqATNmqHJXvCQ5pzx4A9gDoDQcAdXKbmNaAvje/n8B0I2TP6njhlJA1ar8ZwW0nCvQlirlxxzk\nU6dYiOThhx2Tg9xfzZu7dq6Pj2cEbsMG7xOplZDAalZ08GDiY06fZj22UaPYUOrYMb0Mwg34OBWy\nFYBhYObMOBEZaIwZYD/JXGOMAfC5XcnHAxgoItOSOmaouGVEgAYN+Fi8ezeQPbtj26ZNQM6cTF0D\nAOzYARQoABQsmPyB16xhwLNTJ+CZZ4DcuR0nXLmSz98LFgA2G1CkCGu83HabI5prDNCwIWu4KBmC\nnTsZ8xk+HHjpJddt8fHA2LF8wjx92nVbsWLA4sW8fJTQR2vL+Ii5c8W7nsarVnH6ae7cnHqf3AzN\njh0dtctz5WIVxQkTHBOOChYU6dOHHTpCojOHkh5UrszMVmdWr3aUAWrYUGTzZj68LVrEaqK5c4u0\nbRsYeRXfA52hmnbi4vhjKl8+meq0+/ezGUXZsizIDvDXtm6d5/Fnz7ILUa9edJ906uRQ9OXL806S\n2vZxSljTrx8Ljx07xkundWteNkWLsjmIJxeM5c6xegQooY0q9zRw7pzI9OkiDz7IT+iHH5IYfOmS\nSLVqtL63b+eva9o0zis3htUS3bHmn1u1zkU4sWj5cq3JoiTJli28dCpV4t+8eUUGDhS5eDHxfS5e\nZGevRo0SKv8vvxRp1cq1SZYS3KhyTwUHD4q0bEnvCiBSoABreyQakLLZRDp0oBL/5RfXbefPs2ND\n2bIJ6/g2bMjKYhrpUlKIzSZy++1sbPX++zREvOHLL3lNL1zoWDd2LNcZw4nMztuU4EWVeyp48kl2\nz3nzTbrQkzWiBw3iRzh4sOftc+ZIgtZKe/dy3Ucf+UpsJYNx5gxth5Rw7Rozu6pXZwhn1ixme7Vo\nwS6IVapQyffvryGeYEeVewo5eJAW+4sverlDTAwjVW3aJG6B22z0vZcu7XDaf/ghP3ZP9V0UxY9M\nmMBL79VXGfKpXdvRS+DSJfZnAUS6dg2snErSeKvcda65naFDmXX42mte7vDzz8D585zen1iZR2OA\nDz7gnPEJE5jmOHEiUy+Agv4AAA/OSURBVBhLlfKR5IriHZ07A1Wq8FovUwb45RdHyYwcOXiJvvUW\nL9FVqwIrq5J2VLkDOHOGZVg6dUqBzp0wAShalFXDkuL++4G77gI++oi1XHbtcmqsqSjpR2QkJyc3\nbw4sWgTkz++63RigXz9Oq3jrLUehPCU0UeUOXvCXLzt6NyfLyZPshtylC38xSWFZ7/v2cXzWrMDD\nD6dRYkVJHQ0bUrEXL+55e/bsQP/+LKsxZ076yqb4lgyv3C9f5oy/1q0dxROTZdo0IC4O6NrVu/H3\n3QfUrs0KY23aOGajKkoQ0r07Z8L26cPLXAlNMrxyHzeO07XffjsFO02YwLro3t4NjKFbxhjgqadS\nJaeipBdRUcCgQSzt/913iY/bto1VURcuTD/ZFO/xqraMPwiG2jKxsUC5ckCJEsDvv3sYsG8fA6fP\nPccrHuAVf9ttwOefpyD6aufUKdaeUZQgR4SKe+/ehDWVAJa4btiQBUlz5ODvp1q1wMia0fBls46w\npX9/ekreeSeRAW+8wQpN7duzxC7AVIKICEZfU4oqdiVEMAYYPBg4epR9C44fd2zbvx9o2pQum6VL\nWSn6/vv5W3Lm2DHWwFMChDf5kv54BTrPfdkyTtro3j2RASdOMPG9WjUObNCA9VRLlOA0VkXJAPTu\nzclO2bKJvPyyyPr1ImXKiOTJI7JpE8f8/TdnzFapwhmzR48ylz5rVubNz5kT2PcQbkAnMSXOyZMs\ntFSxomMSRwK++IIfz99/s1ZMpkwit9zCdZMnp6u8ihJIdu4UeeIJR227nDlF1qxxHbNkiUhUlMit\nt1KpR0ZynzvvZIHTY8cCI3s4oso9EWw2kQceEMmc2WF5eBxUubJIrVqOdYsXi+TIwSv78uV0kVVR\ngok9e1iaI7HqkuPHc+brE0+I/Psv123dynWtW2spJV+hyj0RRozgux4+PIlBa9Zw0OjRrut37BD5\n80+/yqcooYynekzDhiX8OW3dKtK5M28ER4+m/DwxMSIzZiSsyZcRUOXuge++o3clWSvi6adFsmdP\neXUmRVESEB8vcu+9/EktXCjSpQvDWLly0arPm1fk++9TZtm//z61V79+fhM7aPFWuWeIbJn4eM4+\n7d6d6VsTJyZeDgaXL3OSUocO7IitKEqaiIhgvnzmzEDLlsDMmfw97t0LbN4MVKoEdOsGtGoFHDqU\n/PGuXOGs8syZOX3EYxqzGxcuuPaXzwiEvXK/dAl46CHgs8+Yrr5gAZAnTxI7zJjBq6BHj3STUVHC\nnWLF+NN65x32I/7kE9a2ufVW4LffgBEjqKRbtqTyTooJEzhlZOZMFkDr0gU4ezbx8cuWcT5LzZrA\nuXO+fV9BjTfmvT9e6eGWOX2a9asjItiswCsaNNBGGooSABYupKulR4/Ex8TFiZQrJ3LXXfyJrlvH\nLJ2HH074k7XZ2GohIoLdKzNlYhZzqDc7Q0b3uZ87x17TmTOLzJvn5U7btvEj+eQTv8qmKIpn+vTh\nT3DSJM/bZ87k9unTHes+/dSRJLFrF/MetmyhwgfYLO3iRQZ0AZE33kif9+IvMrRyv3BBpE4d3ql/\n/tnLnY4epUmQJ0/qwveKoqSZ2FiRu+9m1vE//yTcXq8ee984Z8nExzv60ju/IiNFhgxxteiff57b\nJk70/3vxF94q96gAe4V8zuXLnAq9fj3w44+s9pgsZ8+yyPWRI8CSJcDNN/tdTkVREhIVBUydyrp8\nHTsCf/7pqGvzxx98jRjhKPUEMGA7axbLQNls3BYZSX++e22/YcNY8Oypp4DSpYH69dPvvaU3YVU4\n7MoV4IEHgBUrgClTgEce8WKnS5eAZs2AjRuBefP4v6IoAWXBAmbP3HQTK2Y/+CAweTL73Rw4AOTM\nmfpjnzwJ1KnD47z7LoO8mTL5TnZ/k+EKh1kW+4oVwPffe6nYY2OBdu2AdeuY/qiKXVGCgvvuY5ZL\nhw7A8uWs0zdvHjPe0qLYAaBgQSA6mjrigw+AevWA7dt9InZQERZumUuXqNhXrWKaVJcuXu44dSrd\nMN9+SyWvKErQ0LgxX/HxwNq1tNp79vTNsfPmBSZN4hPBs88C1asD8+cn3zUzlAh5t8ylS3x8W72a\nX5bXlXhFgBo1gGvXgK1bk5jVpChKOHP8OFXBHXdQwQc7PnXLGGNaGmN2GmN2G2MS9CwyxjxhjDlp\njNlsf6Vbu6HevRlkmTIlhSXWf/8d2LQJeOUVVeyKkoEpXBh44gn2lj1yJNDS+I5klbsxJhLAVwDu\nA1AJQCdjTCUPQ6eLSFX7a4yP5fTIlStU6o8/7qWP3ZlhwzhF7rHH/CKboiihQ7duzLSZODHQkvgO\nbyz3WgB2i8geEbkOYBqAtv4VyztmzaJbplu3FO64Zw8wezbwzDNAtmx+kU1RlNChfHm2FRw/nh7b\ncMAb5X4LgINOy4fs69xpb4zZYoyZYYwp7hPpkuH774GSJYEGDVK448iRTIR9/nm/yKUoSujxxBNs\nkbx2baAl8Q2+SoX8GUApEbkDwK8Avvc0yBjT0xgTbYyJPnnyZJpOePgwE10ef5yTGLzmwgVgzBjO\nkLjF0z1KUZSMSMeOnDA1fnygJfEN3qjFwwCcLfFi9nU3EJHTInLNvjgGQA1PBxKRb0SkpojULFiw\nYGrkvcGkSXx86to1mYF79gBbtjgaXI8fz6qPL7+cpvMrihJe5MoFtG/PKS+WukgJIsDffwP9+wdH\n1o03ee7rAZQ3xpQGlfqjADo7DzDGFBGRo/bFNgB2+FRKN0TokqlXj76yRDlwgPOYL15kRkyJEsD5\n89yxVi1/iqgoSgjSvTuDqrNnO7Lvrl4F/v0XqFzZs5fg3385v+bHH4GdO7mucGFg/34gS5b0k92d\nZC13EYkD0AvAIlBp/yAi24wxA4wxbezDXjLGbDPG/AXgJQBP+EtgANiwAdixI5lAqghnPNhswLhx\nnIpWvz47A3z4oT/FUxQlRGnUCChVig/4Bw+yNEHx4syBr1wZGD3aUW9+7Vpa+hUrAh9/TC/vqFEs\nk3D8ODB9eiDfCUKzKmSvXiJZstjk7M+/s9t1o0YJ26t/9x3Lv3ldyF1RFIUt/IxhVcmICJG2bdl7\nuXp1qpT8+Vl1FmAR2b59RY4ccexvs4lUqiRSrZp/2kLAy6qQITdD9fqVOBS9OR5No37D9LPNgQIF\neCstUgRYvJitWY4epYVepQqwcmUKI66KomRkDh1i97Z77mFCXalSXC/CuY9DhwK7dgFPP82Gbbly\nJTzGt9/ScbBiBZ8GnImPZ7JeavF2hmrIWe6zOkwWQGRekadERo0SuXJFZM0akXz5RG6+WWTzZt5q\ns2YV2bkzVedQFEVJC1eu0MJ/8EHX9UePitx5p8iCBak/NsK1nvvFOs1QNfocWuz4Gshiv/3Vrs2q\nYc2bs5bn1atsmlqhQmCFVRQlQ5ItGwuSffwxE/bKlGGp4aZNgX370l7Z0htCzi2TJAcPssNu/vys\nE5qWZx9FUZQ0cOQIJ1n26sW68U2aMLNm/ny6fFKLt26ZkLPck6R4cSaa2myq2BVFCShFi7Lm1dix\nDP3t3MluUWlR7Ckh/CKNERGuPbgURVECxKuvcprNtm2shZWe/YBUCyqKoviJGjUY/qtaFbj33vQ9\ntyp3RVEUP/L664E5b/i5ZRRFURRV7oqiKOGIKndFUZQwRJW7oihKGKLKXVEUJQxR5a4oihKGqHJX\nFEUJQ1S5K4qihCEBKxxmjDkJYH8qdy8A4JQPxfElwSpbsMoFBK9swSoXELyyBatcQPjIVlJEkm1C\nHTDlnhaMMdHeVEULBMEqW7DKBQSvbMEqFxC8sgWrXEDGk03dMoqiKGGIKndFUZQwJFSV+zeBFiAJ\nglW2YJULCF7ZglUuIHhlC1a5gAwmW0j63BVFUZSkCVXLXVEURUmCkFPuxpiWxpidxpjdxpi3AyzL\nOGPMCWPMVqd1+Ywxvxpj/rX/zRsAuYobY5YbY7YbY7YZY14OBtmMMVmNMeuMMX/Z5epvX1/aGLPW\n/p1ON8ZkTk+53GSMNMZsMsbMCxbZjDH7jDF/G2M2G2Oi7esCfp3Z5chjjJlhjPnHGLPDGFM30LIZ\nYyraPyvrdcEY80qg5XKS71X79b/VGDPV/rvw+XUWUsrdGBMJ4CsA9wGoBKCTMaZSAEUaD6Cl27q3\nASwVkfIAltqX05s4AL1FpBKAOgBesH9OgZbtGoAmInIngKoAWhpj6gD4FMBQESkH4CyAHukslzMv\nA9jhtBwssjUWkapO6XKB/i4thgNYKCK3ArgT/OwCKpuI7LR/VlUB1ABwBcCsQMsFAMaYWwC8BKCm\niFQBEAngUfjjOhORkHkBqAtgkdNyHwB9AixTKQBbnZZ3Aihi/78IgJ1B8LnNAdAsmGQDkB3ARgC1\nwckbUZ6+43SWqRj4o28CYB4AEwyyAdgHoIDbuoB/lwByA9gLe+wumGRzkqU5gNXBIheAWwAcBJAP\n7IQ3D0ALf1xnIWW5w/HBWByyrwsmCovIUfv/xwAUDqQwxphSAKoBWIsgkM3u9tgM4ASAXwH8B+Cc\niMTZhwTyOx0G4E0ANvtyfgSHbAJgsTFmgzGmp31dwL9LAKUBnATwnd2VNcYYkyNIZLN4FMBU+/8B\nl0tEDgMYAuAAgKMAzgPYAD9cZ6Gm3EMK4W04YOlIxpicAH4C8IqIXHDeFijZRCRe+LhcDEAtALem\ntwyeMMa0BnBCRDYEWhYP3C0i1UF35AvGmIbOGwN4nUUBqA5glIhUA3AZbq6OQP4G7H7rNgB+dN8W\nKLnsfv624I2xKIAcSOja9QmhptwPAyjutFzMvi6YOG6MKQIA9r8nAiGEMSYTqNgni8jMYJINAETk\nHIDl4CNoHmOM1aw9UN9pfQBtjDH7AEwDXTPDg0E2u7UHETkB+o5rITi+y0MADonIWvvyDFDZB4Ns\nAG+GG0XkuH05GOS6F8BeETkpIrEAZoLXns+vs1BT7usBlLdHljODj1xzAyyTO3MBdLP/3w30d6cr\nxhgDYCyAHSLyRbDIZowpaIzJY/8/GxgH2AEq+YcDJRcAiEgfESkmIqXA62qZiHQJtGzGmBzGmFzW\n/6APeSuC4DoTkWMADhpjKtpXNQWwPRhks9MJDpcMEBxyHQBQxxiT3f47tT4z319ngQp0pCEg0QrA\nLtBX2zfAskwF/WaxoBXTA/TTLgXwL4AlAPIFQK67wUfOLQA221+tAi0bgDsAbLLLtRVAP/v6MgDW\nAdgNPkJnCfD3eg+AecEgm/38f9lf26xrPtDfpZN8VQFE27/T2QDyBoNsoLvjNIDcTusCLpddjv4A\n/rH/BiYCyOKP60xnqCqKooQhoeaWURRFUbxAlbuiKEoYospdURQlDFHlriiKEoaoclcURQlDVLkr\niqKEIarcFUVRwhBV7oqiKGHI/wFjhmHKVPIZSAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"YYTWjj76_GYN","colab_type":"code","outputId":"aa5076a7-9f04-4e25-b4a1-7aca60fec3a8","colab":{"base_uri":"https://localhost:8080/","height":128}},"source":[""],"execution_count":0,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-1209fb88269b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python -tt scriptname.py\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"woybl9dm7SBW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}